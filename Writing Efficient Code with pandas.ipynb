{"cells":[{"source":"# Writing Efficient Code with pandas","metadata":{},"id":"20c73b4c-626a-4674-b2ca-0f4c47f60628","cell_type":"markdown"},{"source":"## Measuring time I \nIn the lecture slides, you saw how the time.time() function can be loaded and used to assess the time required to perform a basic mathematical operation.\n\nNow, you will use the same strategy to assess two different methods for solving a similar problem: calculate the sum of squares of all the positive integers from 1 to 1 million (1,000,000).\n\nSimilar to what you saw in the video, you will compare two methods; one that uses brute force and one more mathematically sophisticated.\n\nIn the function formula, we use the standard formula\n\n \nwhere N=1,000,000.\n\nIn the function brute_force we loop over each number from 1 to 1 million and add it to the result.","metadata":{},"cell_type":"markdown","id":"8261d262-8d3a-4573-953d-24649faec3b6"},{"source":"# Calculate the result of the problem using formula() and print the time required\nN = 1000000\nfm_start_time = time.time()\nfirst_method = formula(N)\nprint(\"Time using formula: {} sec\".format(time.time() - fm_start_time))\n\n# Calculate the result of the problem using brute_force() and print the time required\nsm_start_time = time.time()\nsecond_method = brute_force(N)\nprint(\"Time using the brute force: {} sec\".format(time.time() - sm_start_time))","metadata":{},"cell_type":"code","id":"d1f5f6e2-249f-4e91-9841-4319e753b0a3","execution_count":null,"outputs":[]},{"source":"## Measuring time II\nAs we discussed in the lectures, in the majority of cases, a list comprehension is faster than a for loop.\n\nIn this demonstration, you will see a case where a list comprehension and a for loop have so small difference in efficiency that choosing either method will perform this simple task instantly.\n\nIn the list words, there are random words downloaded from the Internet. We are interested to create another list called listlet in which we only keep the words that start with the letter b.\n\nIn case you are not familiar with dealing with strings in Python, each string has the .startswith() attribute, which returns a True/False statement whether the string starts with a specific letter/phrase or not.","metadata":{},"cell_type":"markdown","id":"dfcf79f9-77ce-419b-84d2-ee19fcae80ef"},{"source":"# Store the time before the execution\nstart_time = time.time()\n\n# Execute the operation\nletlist = [wrd for wrd in words if wrd.startswith('b')]\n\n# Store and print the difference between the start and the current time\ntotal_time_lc = time.time() - start_time\nprint('Time using list comprehension: {} sec'.format(total_time_lc))\n\n# Store the time before the execution\nstart_time = time.time()\n\n# Execute the operation\nletlist = []\nfor wrd in words:\n    if wrd.startswith('b'):\n        letlist.append(wrd)\n        \n# Print the difference between the start and the current time\ntotal_time_fl = time.time() - start_time\nprint('Time using for loop: {} sec'.format(total_time_fl))","metadata":{},"cell_type":"code","id":"6c459745-2190-4aa8-b622-4febbcb06b87","execution_count":null,"outputs":[]},{"source":"## Row selection: loc[] vs iloc[]\nA big part of working with DataFrames is to locate specific entries in the dataset. You can locate rows in two ways:\n\nBy a specific value of a column (feature).\nBy the index of the rows (index). In this exercise, we will focus on the second way.\nIf you have previous experience with pandas, you should be familiar with the .loc and .iloc indexers, which stands for 'location' and 'index location' respectively. In most cases, the indices will be the same as the position of each row in the Dataframe (e.g. the row with index 13 will be the 14th entry).\n\nWhile we can use both functions to perform the same task, we are interested in which is the most efficient in terms of speed.","metadata":{},"cell_type":"markdown","id":"35cfaad7-e2cc-4202-b060-35c41e45c02e"},{"source":"# Define the range of rows to select: row_nums\nrow_nums = range(0, 1000)\n\n# Select the rows using .loc[] and row_nums and record the time before and after\nloc_start_time = time.time()\nrows = poker_hands.loc[row_nums]\nloc_end_time = time.time()\n\n# Print the time it took to select the rows using .loc\nprint(\"Time using .loc[]: {} sec\".format(loc_end_time - loc_start_time))\n\n# Select the rows using .iloc[] and row_nums and record the time before and after\niloc_start_time = time.time()\nrows = poker_hands.iloc[row_nums]\niloc_end_time = time.time()\n\n# Print the time it took to select the rows using .iloc\nprint(\"Time using .iloc[]: {} sec\".format(iloc_end_time - iloc_start_time))","metadata":{},"cell_type":"code","id":"d282dd4b-91bd-4c11-ac63-50738c08ab5c","execution_count":null,"outputs":[]},{"source":"## Column selection: .iloc[] vs by name\nIn the previous exercise, you saw how the .loc[] and .iloc[] functions can be used to locate specific rows of a DataFrame (based on the index). Turns out, the .iloc[] function performs a lot faster (~ 2 times) for this task!\n\nAnother important task is to find the faster function to select the targeted features (columns) of a DataFrame. In this exercise, we will compare the following:\n\nusing the index locator .iloc()\nusing the names of the columns While we can use both functions to perform the same task, we are interested in which is the most efficient in terms of speed.\nIn this exercise, you will continue working with the poker data which is stored in poker_hands. Take a second to examine the structure of this DataFrame by calling poker_hands.head() in the console!\n\n","metadata":{},"cell_type":"markdown","id":"8f1fb298-15a1-483e-b0ee-4cd09e75a4e2"},{"source":"# Use .iloc to select the first, fourth, fifth, seventh and eighth column and record the times before and after\niloc_start_time = time.time()\ncols = poker_hands.iloc[:,[0,3,4,6,7]]\niloc_end_time = time.time()\n\n# Print the time it took\nprint(\"Time using .iloc[] : {} sec\".format(iloc_end_time - iloc_start_time))\n\n# Use simple column selection to select the first, fourth, fifth, seventh and eighth column and record the times before and after\nnames_start_time = time.time()\ncols = poker_hands[['S1', 'S2', 'R2', 'R3', 'S4']]\nnames_end_time = time.time()\n\n# Print the time it took\nprint(\"Time using selection by name : {} sec\".format(names_end_time-names_start_time))","metadata":{},"cell_type":"code","id":"9d72c36d-ecb8-458e-a33d-c4929f6b2713","execution_count":null,"outputs":[]},{"source":"## Random row selection\nIn this exercise, you will compare the two methods described for selecting random rows (entries) with replacement in a pandas DataFrame:\n\nThe built-in pandas function .random()\nThe NumPy random integer number generator np.random.randint()\nGenerally, in the fields of statistics and machine learning, when we need to train an algorithm, we train the algorithm on the 75% of the available data and then test the performance on the remaining 25% of the data.\n\nFor this exercise, we will randomly sample the 75% percent of all the played poker hands available, using each of the above methods, and check which method is more efficient in terms of speed.","metadata":{},"cell_type":"markdown","id":"a9efc53c-9b29-4bfe-b3a3-2baea1102c53"},{"source":"# Extract number of rows in dataset\nN=poker_hands.shape[0]\n\n# Select and time the selection of the 75% of the dataset's rows\nrand_start_time = time.time()\npoker_hands.iloc[np.random.randint(low=0, high=N, size=int(0.75 * N))]\nprint(\"Time using Numpy: {} sec\".format(time.time() - rand_start_time))\n\n# Select and time the selection of the 75% of the dataset's rows using sample()\nsamp_start_time = time.time()\npoker_hands.sample(int(0.75 * N), axis=0, replace = True)\nprint(\"Time using .sample: {} sec\".format(time.time() - samp_start_time))","metadata":{},"cell_type":"code","id":"b48e514e-e025-4cb4-ba01-fdd949b0ae92","execution_count":null,"outputs":[]},{"source":"## Random column selection\nIn the previous exercise, we examined two ways to select random rows from a pandas DataFrame. We can use the same functions to randomly select columns in a pandas DataFrame.\n\nTo randomly select 4 columns out of the poker dataset, you will use the following two functions:\n\nThe built-in pandas function .sample()\nThe NumPy random integer number generator np.random.randint()","metadata":{},"cell_type":"markdown","id":"cf4fcad4-9494-4168-af83-24c92c4f7b42"},{"source":"# Extract number of columns in dataset\nD=poker_hands.shape[1]\n\n# Select and time the selection of 4 of the dataset's columns using NumPy\nnp_start_time = time.time()\npoker_hands.iloc[:,np.random.randint(low=0, high=D, size=4)]\nprint(\"Time using NymPy's random.randint(): {} sec\".format(time.time() - np_start_time))\n\n# Select and time the selection of 4 of the dataset's columns using pandas\npd_start_time = time.time()\npoker_hands.sample(4, axis=1)\nprint(\"Time using panda's .sample(): {} sec\".format(time.time() - pd_start_time))","metadata":{},"cell_type":"code","id":"4a536e04-1ea9-4db7-b32a-7328365216d5","execution_count":null,"outputs":[]},{"source":"## Replacing scalar values I\nIn this exercise, we will replace a list of values in our dataset by using the .replace() method with another list of desired values.\n\nWe will apply the functions in the poker_hands DataFrame. Remember that in the poker_hands DataFrame, each row of columns R1 to R5 represents the rank of each card from a player's poker hand spanning from 1 (Ace) to 13 (King). The Class feature classifies each hand as a category, and the Explanation feature briefly explains each hand.\n\nThe poker_hands DataFrame is already loaded for you, and you can explore the features Class and Explanation.\n\nRemember you can always explore the dataset and see how it changes in the IPython Shell, and refer to the slides in the Slides tab.","metadata":{},"cell_type":"markdown","id":"4d1fd0ba-513e-4b8e-80ad-7b566f7fb9fb"},{"source":"# Replace Class 1 to -2 \npoker_hands['Class'].replace(1, -2, inplace=True)\n# Replace Class 2 to -3\npoker_hands[\"Class\"].replace(2, -3, inplace=True)\n\nprint(poker_hands[['Class', 'Explanation']])","metadata":{},"cell_type":"code","id":"55af8af3-84a9-4d33-930a-1cdfa30b95d8","execution_count":null,"outputs":[]},{"source":"## Replace scalar values II\nAs discussed in the video, in a pandas DataFrame, it is possible to replace values in a very intuitive way: we locate the position (row and column) in the Dataframe and assign in the new value you want to replace with. In a more pandas-ian way, the .replace() function is available that performs the same task.\n\nYou will be using the names DataFrame which includes, among others, the most popular names in the US by year, gender and ethnicity.\n\nYour task is to replace all the babies that are classified as FEMALE to GIRL using the following methods:\n\nintuitive scalar replacement\nusing the .replace() function","metadata":{},"cell_type":"markdown","id":"3aa95d9d-2fac-480a-86b3-655495671bef"},{"source":"start_time = time.time()\n\n# Replace all the entries that has 'FEMALE' as a gender with 'GIRL'\nnames['Gender'].loc[names.Gender == 'FEMALE'] = 'GIRL'\n\nprint(\"Time using .loc[]: {} sec\".format(time.time() - start_time))","metadata":{},"cell_type":"code","id":"b0b4de39-1dcc-49ed-8148-9faea8f99e69","execution_count":null,"outputs":[]},{"source":"start_time = time.time()\n\n# Replace all the entries that has 'FEMALE' as a gender with 'GIRL'\nnames['Gender'].replace('FEMALE', 'GIRL', inplace=True)\n\nprint(\"Time using .replace(): {} sec\".format(time.time() - start_time))","metadata":{},"cell_type":"code","id":"02d1b113-6115-4ed8-b2f1-5211c34e9be1","execution_count":null,"outputs":[]},{"source":"## Replace multiple values I\nIn this exercise, you will apply the .replace() function for the task of replacing multiple values with one or more values. You will again use the names dataset which contains, among others, the most popular names in the US by year, gender and Ethnicity.\n\nThus you want to replace all ethnicities classified as black or white non-hispanics to non-hispanic. Remember, the ethnicities are stated in the dataset as follows: ['BLACK NON HISP', 'BLACK NON HISPANIC', 'WHITE NON HISP' , 'WHITE NON HISPANIC'] and should be replaced to 'NON HISPANIC'","metadata":{},"cell_type":"markdown","id":"287065db-de33-4ca0-afa3-94aae0b96533"},{"source":"start_time = time.time()\n\n# Replace all non-Hispanic ethnicities with 'NON HISPANIC'\nnames['Ethnicity'].loc[(names[\"Ethnicity\"] == 'BLACK NON HISP') | \n                      (names[\"Ethnicity\"] == 'BLACK NON HISPANIC') | \n                      (names[\"Ethnicity\"] == 'WHITE NON HISP') | \n                      (names[\"Ethnicity\"] == 'WHITE NON HISPANIC')] = 'NON HISPANIC'\n\nprint(\"Time using .loc[]: sec\".format(time.time() - start_time))","metadata":{},"cell_type":"code","id":"fb7c4051-03ae-418c-9078-8201378aac5b","execution_count":null,"outputs":[]},{"source":"start_time = time.time()\n\n# Replace all non-Hispanic ethnicities with 'NON HISPANIC'\nnames['Ethnicity'].replace(['BLACK NON HISP', 'BLACK NON HISPANIC', 'WHITE NON HISP' , 'WHITE NON HISPANIC'],  'NON HISPANIC', inplace=True)\n\nprint(\"Time using .replace(): {} sec\".format(time.time() - start_time))","metadata":{},"cell_type":"code","id":"03bfd037-3db2-43b1-9817-3d739ccd89e8","execution_count":null,"outputs":[]},{"source":"## Replace multiple values II\nAs discussed in the video, instead of using the .replace() function multiple times to replace multiple values, you can use lists to map the elements you want to replace one to one with those you want to replace them with.\n\nAs you have seen in our popular names dataset, there are two names for the same ethnicity. We want to standardize the naming of each ethnicity by replacing\n\n'ASIAN AND PACI' to 'ASIAN AND PACIFIC ISLANDER'\n'BLACK NON HISP' to 'BLACK NON HISPANIC'\n'WHITE NON HISP' to 'WHITE NON HISPANIC'\nIn the DataFrame names, you are going to replace all the values on the left by the values on the right.","metadata":{},"cell_type":"markdown","id":"24912832-b66b-42dd-9f3f-2a9edc4c0b4a"},{"source":"start_time = time.time()\n\n# Replace ethnicities as instructed\nnames['Ethnicity'].replace(['ASIAN AND PACI','BLACK NON HISP', 'WHITE NON HISP'], ['ASIAN AND PACIFIC ISLANDER','BLACK NON HISPANIC','WHITE NON HISPANIC'], inplace=True)\n\nprint(\"Time using .replace(): {} sec\".format(time.time() - start_time))","metadata":{},"cell_type":"code","id":"f7ed0ed1-fee6-4e91-bb4b-c0930a451271","execution_count":null,"outputs":[]},{"source":"## Replace single values I\nIn this exercise, we will apply the following replacing technique of replacing multiple values using dictionaries on a different dataset.\n\nWe will apply the functions in the data DataFrame. Each row represents the rank of 5 cards from a playing card deck, spanning from 1 (Ace) to 13 (King) (features R1, R2, R3, R4, R5). The feature 'Class' classifies each row to a category (from 0 to 9) and the feature 'Explanation' gives a brief explanation of what each class represents.\n\nThe purpose of this exercise is to categorize the two types of flush in the game ('Royal flush' and 'Straight flush') under the 'Flush' name.","metadata":{},"cell_type":"markdown","id":"177c58d6-856b-4c83-8421-4ffd9aa433be"},{"source":"# Replace Royal flush or Straight flush to Flush\npoker_hands.replace({'Royal flush':'Flush', 'Straight flush':'Flush'}, inplace=True)\nprint(poker_hands['Explanation'].head())","metadata":{},"cell_type":"code","id":"574d1d1b-b886-4a22-832f-b4f675c2d9e2","execution_count":null,"outputs":[]},{"source":"## Replace single values II\nFor this exercise, we will be using the names DataFrame. In this dataset, the column 'Rank' shows the ranking of each name by year. For this exercise, you will use dictionaries to replace the first ranked name of every year as 'FIRST', the second name as 'SECOND' and the third name as 'THIRD'.\n\nYou will use dictionaries to replace one single value per key.\n\nYou can already see the first 5 names of the data, which correspond to the 5 most popular names for all the females belonging to the 'ASIAN AND PACIFIC ISLANDER' ethnicity in 2011.","metadata":{},"cell_type":"markdown","id":"f27202f4-49ae-4826-bd7f-2b576caf986f"},{"source":"# Replace the number rank by a string\nnames['Rank'].replace({1:'FIRST', 2:'SECOND', 3:'THIRD'}, inplace=True)\nprint(names.head())","metadata":{},"cell_type":"code","id":"b0e0a61d-ff73-4522-8911-da9d8413c97f","execution_count":null,"outputs":[]},{"source":"## Create a generator for a pandas DataFrame\nAs you've seen in the video, you can easily create a generator out of a pandas DataFrame. Each time you iterate through it, it will yield two elements:\n\nthe index of the respective row\na pandas Series with all the elements of that row\nYou are going to create a generator over the poker dataset, imported as poker_hands. Then, you will print all the elements of the 2nd row, using the generator.\n\nRemember you can always explore the dataset and see how it changes in the IPython Shell, and refer to the slides in the Slides tab.","metadata":{},"cell_type":"markdown","id":"21fea4d7-97a2-42b0-9f90-9e81d19957ef"},{"source":"# Create a generator over the rows\ngenerator = poker_hands.iterrows()\n\n# Access the elements of the 2nd row\nfirst_element = next(generator)\nsecond_element = next(generator)\nprint(first_element, second_element)","metadata":{},"cell_type":"code","id":"7e6edcbb-95ed-441f-a964-c4da27b90b4e","execution_count":null,"outputs":[]},{"source":"## The iterrows() function for looping\nYou just saw how to create a generator out of a pandas DataFrame. You will now use this generator and see how to take advantage of that method of looping through a pandas DataFrame, still using the poker_hands dataset.\n\nSpecifically, we want the sum of the ranks of all the cards, if the index of the hand is an odd number. The ranks of the cards are located in the odd columns of the DataFrame.","metadata":{},"cell_type":"markdown","id":"dab1df81-c396-44ec-b9f0-56b0d98fa42e"},{"source":"data_generator = poker_hands.iterrows()\n\nfor index, values in data_generator:\n  \t# Check if index is odd\n    if index%2==1:\n      \t# Sum the ranks of all the cards\n        hand_sum = sum([values[1], values[3], values[5], values[7], values[9]])","metadata":{},"cell_type":"code","id":"c761ce68-4605-425b-b8a9-5e57f76279cc","execution_count":null,"outputs":[]},{"source":"## .apply() function in every cell\nAs you saw in the lesson, you can use .apply() to map a function to every cell of the DataFrame, regardless the column or the row.\n\nYou're going to try it out on the poker_hands dataset. You will use .apply() to square every cell of the DataFrame. The native Python way to square a number n is n**2.","metadata":{},"cell_type":"markdown","id":"6f3fe9bc-c653-4a80-9f26-a91de9644744"},{"source":"# Define the lambda transformation\nget_square = lambda x: x**2\n\n# Apply the transformation\ndata_sum = poker_hands.apply(lambda x: x**2)\nprint(data_sum.head())","metadata":{},"cell_type":"code","id":"e4f8a78a-55da-4ce3-bae9-8b4bf271aaff","execution_count":null,"outputs":[]},{"source":"## .apply() for rows iteration\n.apply() is a very useful to iterate through the rows of a DataFrame and apply a specific function.\n\nYou will work on a subset of the poker_hands dataset, which includes only the rank of all the five cards of each hand in each row (this subset is generated for you in the script). You're going to get the variance of every hand for all ranks, and every rank for all hands.","metadata":{},"cell_type":"markdown","id":"42d9d490-150d-4066-9c90-7d743794dfe7"},{"source":"# Define the lambda transformation\nget_variance = lambda x: np.var(x)\n\n# Apply the transformation\ndata_tr = poker_hands[['R1', 'R2', 'R3', 'R4', 'R5']].apply(get_variance, axis=1)\nprint(data_tr.head())","metadata":{},"cell_type":"code","id":"5f117cfd-6ea6-46f5-ac9e-36898f6649c4","execution_count":null,"outputs":[]},{"source":"get_variance = lambda x: np.var(x)\n\n# Apply the transformation\ndata_tr = poker_hands[['R1', 'R2', 'R3', 'R4', 'R5']].apply(get_variance, axis=0)\nprint(data_tr.head())","metadata":{},"cell_type":"code","id":"64c64809-1e44-4c6f-b517-4ae469e44b2c","execution_count":null,"outputs":[]},{"source":"## pandas vectorization in action\nIn this exercise, you will apply vectorization over pandas series to:\n\ncalculate the mean rank of all the cards in each hand (row)\ncalculate the mean rank of each of the 5 cards in each hand (column)\nYou will use the poker_hands dataset once again to compare both methods' efficiency.","metadata":{},"cell_type":"markdown","id":"e1bfdb93-ce94-4135-8140-603b334d72fc"},{"source":"# Calculate the mean rank in each hand\nrow_start_time = time.time()\nmean_r = poker_hands[['R1', 'R2', 'R3', 'R4', 'R5']].mean(axis=1)\nprint(\"Time using pandas vectorization for rows: {} sec\".format(time.time() - row_start_time))\nprint(mean_r.head())\n\n# Calculate the mean rank of each of the 5 card in all hands\ncol_start_time = time.time()\nmean_c = poker_hands[['R1', 'R2', 'R3', 'R4', 'R5']].mean(axis=0)\nprint(\"Time using pandas vectorization for columns: {} sec\".format(time.time() - col_start_time))\nprint(mean_c.head())","metadata":{},"cell_type":"code","id":"146e1d84-e1fc-4cfd-9cda-dfadcdd4441c","execution_count":null,"outputs":[]},{"source":"## Vectorization methods for looping a DataFrame\nNow that you're familiar with vectorization in pandas and NumPy, you're going to compare their respective performances yourself.\n\nYour task is to calculate the variance of all the hands in each hand using the vectorization over pandas Series and then modify your code using the vectorization over Numpy ndarrays method.","metadata":{},"cell_type":"markdown","id":"65a88f08-e090-44f7-99a9-263d7bd8a13d"},{"source":"# Calculate the variance in each hand\nstart_time = time.time()\npoker_var = poker_hands[['R1','R2','R3','R4','R5']].var(axis=1)\nprint(\"Time using pandas vectorization: {} sec\".format(time.time() - start_time))\nprint(poker_var.head())","metadata":{},"cell_type":"code","id":"d5e73654-3ef3-45d1-8da2-e20dd7b78abd","execution_count":null,"outputs":[]},{"source":"# Calculate the variance in each hand\nstart_time = time.time()\npoker_var = poker_hands[['R1', 'R2', 'R3', 'R4', 'R5']].values.var(axis=1, ddof=1)\nprint(\"Time using NumPy vectorization: {} sec\".format(time.time() - start_time))\nprint(poker_var[0:5])","metadata":{},"cell_type":"code","id":"dd3250b0-a52b-4c7a-a420-b23a0378e6de","execution_count":null,"outputs":[]},{"source":"## The min-max normalization using .transform()\nA very common operation is the min-max normalization. It consists in rescaling our value of interest by deducting the minimum value and dividing the result by the difference between the maximum and the minimum value. For example, to rescale student's weight data spanning from 160 pounds to 200 pounds, you subtract 160 from each student's weight and divide the result by 40 (200 - 160).\n\nYou're going to define and apply the min-max normalization to all the numerical variables in the restaurant data. You will first group the entries by the time the meal took place (Lunch or Dinner) and then apply the normalization to each group separately.\n\nRemember you can always explore the dataset and see how it changes in the IPython Shell, and refer to the slides in the Slides tab.","metadata":{},"cell_type":"markdown","id":"c1c0058d-ba12-483e-b55e-b3262a1e108f"},{"source":"# Define the min-max transformation\nmin_max_tr = lambda x: (x - x.min()) / (x.max() - x.min())\n\n# Group the data according to the time\nrestaurant_grouped = restaurant_data.groupby('time')\n\n# Apply the transformation\nrestaurant_min_max_group = restaurant_grouped.transform(min_max_tr)\nprint(restaurant_min_max_group.head())","metadata":{},"cell_type":"code","id":"0bfac7e7-d092-4e71-863b-6b548b420602","execution_count":null,"outputs":[]},{"source":"## Transforming values to probabilities\nIn this exercise, we will apply a probability distribution function to a pandas DataFrame with group related parameters by transforming the tip variable to probabilities.\n\nThe transformation will be a exponential transformation. The exponential distribution is defined as\n\n\nwhere λ (lambda) is the mean of the group that the observation x belongs to.\n\nYou're going to apply the exponential distribution transformation to the size of each table in the dataset, after grouping the data according to the time of the day the meal took place. Remember to use each group's mean for the value of λ.\n\nIn Python, you can use the exponential as np.exp() from the NumPy library and the mean value as .mean().","metadata":{},"cell_type":"markdown","id":"91f694f2-5886-4c16-8d9b-8b03f7554719"},{"source":"# Define the exponential transformation\nexp_tr = lambda x: np.exp(-x.mean()*x) * x.mean()\n\n# Group the data according to the time\nrestaurant_grouped = restaurant_data.groupby('time')\n\n# Apply the transformation\nrestaurant_exp_group = restaurant_grouped['tip'].transform(exp_tr)\nprint(restaurant_exp_group.head())","metadata":{},"cell_type":"code","id":"e004394d-0ca8-4ad8-b91b-c2dc0b7a0751","execution_count":null,"outputs":[]},{"source":"## Validation of normalization\nFor this exercise, we will perform a z-score normalization and verify that it was performed correctly.\n\nA distinct characteristic of normalized values is that they have a mean equal to zero and standard deviation equal to one.\n\nAfter you apply the normalization transformation, you can group again on the same variable, and then check the mean and the standard deviation of each group.\n\nYou will apply the normalization transformation to every numeric variable in the poker_grouped dataset, which is the poker_hands dataset grouped by Class.","metadata":{},"cell_type":"markdown","id":"621ce42d-dcfa-44f8-b231-1b2f4576b317"},{"source":"zscore = lambda x: (x - x.mean()) / x.std()\n\n# Apply the transformation\npoker_trans = poker_grouped.transform(zscore)\n\n# Re-group the grouped object\npoker_regrouped = poker_trans.groupby(poker_hands['Class'])\n\n# Print each group's means and standard deviation\nprint(np.round(poker_regrouped.mean(), 3))\nprint(poker_regrouped.std())","metadata":{},"cell_type":"code","id":"d646a081-f423-492b-a03d-f6143ce80b0f","execution_count":null,"outputs":[]},{"source":"## Identifying missing values\nThe first step before missing value imputation is to identify if there are missing values in our data, and if so, from which group they arise.\n\nFor the same restaurant_data data you encountered in the lesson, an employee erased by mistake the tips left in 65 tables. The question at stake is how many missing entries came from tables that smokers where present vs tables with no-smokers present.\n\nYour task is to group both datasets according to the smoker variable, count the number or present values and then calculate the difference.\n\nWe're imputing tips to get you to practice the concepts taught in the lesson. From an ethical standpoint, you should not impute financial data in real life, as it could be considered fraud.","metadata":{},"cell_type":"markdown","id":"5d233caf-8e7d-4d77-acb6-8e116d17a5e6"},{"source":"# Group both objects according to smoke condition\nrestaurant_nan_grouped = restaurant_nan.groupby('smoker')\n\n# Store the number of present values\nrestaurant_nan_nval = restaurant_nan_grouped['tip'].count()\n\n# Print the group-wise missing entries\nprint(restaurant_nan_grouped['total_bill'].count() - restaurant_nan_nval)","metadata":{},"cell_type":"code","id":"61528cb2-5766-4d58-b876-44ae947c6efd","execution_count":null,"outputs":[]},{"source":"## Missing value imputation\nAs the majority of the real world data contain missing entries, replacing these entries with sensible values can increase the insight you can get from our data.\n\nIn the restaurant dataset, the \"total_bill\" column has some missing entries, meaning that you have not recorded how much some tables have paid. Your task in this exercise is to replace the missing entries with the median value of the amount paid, according to whether the entry was recorded on lunch or dinner (time variable).","metadata":{},"cell_type":"markdown","id":"5976f776-dc62-4d45-a8a9-6ff53542e3f1"},{"source":"# Define the lambda function\nmissing_trans = lambda x: x.fillna(x.median())\n\n# Group the data according to time\nrestaurant_grouped = restaurant_data.groupby('time')\n\n# Apply the transformation\nrestaurant_impute = restaurant_grouped.transform(missing_trans)\nprint(restaurant_impute.head())","metadata":{},"cell_type":"code","id":"e6fbfaae-c17a-47ad-9061-fb4bd1a7cb40","execution_count":null,"outputs":[]},{"source":"## Data filtration\nAs you noticed in the video lesson, you may need to filter your data for various reasons.\n\nIn this exercise, you will use filtering to select a specific part of our DataFrame:\n\nby the number of entries recorded in each day of the week\nby the mean amount of money the customers paid to the restaurant each day of the week","metadata":{},"cell_type":"markdown","id":"431f862f-7bf2-437d-82ac-8af069630978"},{"source":"# Filter the days where the count of total_bill is greater than $40\ntotal_bill_40 = restaurant_data.groupby('day').filter(lambda x: x['size'].count() > 40)\n\n# Print the number of tables where total_bill is greater than $40\nprint('Number of tables where total_bill is greater than $40:', total_bill_40.shape[0])","metadata":{},"cell_type":"code","id":"90f1ac60-34f0-4a07-b4aa-e8de8b914495","execution_count":null,"outputs":[]},{"source":"# Filter the days where the count of total_bill is greater than $40\ntotal_bill_40 = restaurant_data.groupby('day').filter(lambda x: x['total_bill'].count() > 40)\n\n# Select only the entries that have a mean total_bill greater than $20\ntotal_bill_20 = total_bill_40.groupby('day').filter(lambda x : x['total_bill'].mean() > 20)\n\n# Print days of the week that have a mean total_bill greater than $20\nprint('Days of the week that have a mean total_bill greater than $20:', total_bill_20.day.unique())","metadata":{},"cell_type":"code","id":"8346accd-5d08-4680-ae53-c522bce74586","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}