{"cells":[{"source":"# Unit Testing for Data Science in Python","metadata":{},"id":"20c73b4c-626a-4674-b2ca-0f4c47f60628","cell_type":"markdown"},{"source":"## Your first unit test using pytest\nThe data file containing housing area and prices uses commas as thousands separators, e.g. \"2,081\" or \"314,942\", as you can see in the IPython Shell.\n\nThe convert_to_int() function takes a comma separated integer string as argument, and returns the integer. Therefore, the expected return value of convert_to_int(\"2,081\") is the integer 2081.\n\nThis function is defined in the module preprocessing_helpers.py. But it is not known if the function is working properly.","metadata":{},"cell_type":"markdown","id":"45a2823c-604f-46db-b5eb-652b435592c6"},{"source":"# Import the pytest package\nimport pytest\n\n# Import the function convert_to_int()\nfrom preprocessing_helpers import convert_to_int\n\n# Complete the unit test name by adding a prefix\ndef test_on_string_with_one_comma():\n  # Complete the assert statement\n  assert convert_to_int(\"2,081\") == 2081","metadata":{},"cell_type":"code","id":"02c4ac96-362a-4289-adc1-a2577eef9751","execution_count":null,"outputs":[]},{"source":"## Write an informative test failure message\nThe test result reports become a lot easier to read when you make good use of the optional message argument of the assert statement.\n\nIn a previous exercise, you wrote a test for the convert_to_int() function. The function takes an integer valued string with commas as thousand separators e.g. \"2,081\" as argument and should return the integer 2081.\n\nIn this exercise, you will rewrite the test called test_on_string_with_one_comma() so that it prints an informative message if the test fails.","metadata":{},"cell_type":"markdown","id":"f6ea46d3-9d63-4062-8075-272e59f0662d"},{"source":"import pytest\nfrom preprocessing_helpers import convert_to_int\n\ndef test_on_string_with_one_comma():\n    test_argument = \"2,081\"\n    expected = 2081\n    actual = convert_to_int(test_argument)\n    # Format the string with the actual return value\n    message = \"convert_to_int('2,081') should return the int 2081, but it actually returned {0}\".format(actual)\n    # Write the assert statement which prints message on failure\n    assert actual == expected, message","metadata":{},"cell_type":"code","id":"bbfb2327-0a8d-4e5c-93b7-5aefbdf5ffc8","execution_count":null,"outputs":[]},{"source":"## Testing float return values\nThe get_data_as_numpy_array() function (which was called mystery_function() in one of the previous exercises) takes two arguments: the path to a clean data file and the number of data columns in the file . An example file has been printed out in the IPython console. It contains three rows.\n\nThe function converts the data into a 3x2 NumPy array with dtype=float64. The expected return value has been stored in a variable called expected. Print it out to see it.\n\nThe housing areas are in the first column and the housing prices are in the second column. This array will be the features that will be fed to the linear regression model for learning.\n\nThe return value contains floats. Therefore you have to be especially careful when writing unit tests for this function.","metadata":{},"cell_type":"markdown","id":"8f7de3c9-95a2-4551-991c-0f698d1ebf9e"},{"source":"import numpy as np\nimport pytest\nfrom as_numpy import get_data_as_numpy_array\n\ndef test_on_clean_file():\n  expected = np.array([[2081.0, 314942.0],\n                       [1059.0, 186606.0],\n  \t\t\t\t\t   [1148.0, 206186.0]\n                       ]\n                      )\n  actual = get_data_as_numpy_array(\"example_clean_data.txt\", num_columns=2)\n  message = \"Expected return value: {0}, Actual return value: {1}\".format(expected, actual)\n  # Complete the assert statement\n  assert actual == pytest.approx(expected), message","metadata":{},"cell_type":"code","id":"5746de5e-2ecb-418d-9bdc-f71ecc18a73a","execution_count":null,"outputs":[]},{"source":"## Testing with multiple assert statements\nYou're now going to test the function split_into_training_and_testing_sets() from the models module.\n\nIt takes a n x 2 NumPy array containing housing area and prices as argument. To see an example argument, print the variable example_argument in the IPython console.\n\nThe function returns a 2-tuple of NumPy arrays (training_set, testing_set). The training set contains int(0.75 * n) (approx. 75%) randomly selected rows of the argument array. The testing set contains the remaining rows.\n\nPrint the variable expected_return_value in the IPython console. example_argument had 6 rows. Therefore the training array has int(0.75 * 6) = 4 of its rows and the testing array has the remaining 2 rows.\n\nnumpy as np, pytest and split_into_training_and_testing_sets have been imported for you.","metadata":{},"cell_type":"markdown","id":"8ef70ff3-22f0-4f51-b55b-0f102f443b62"},{"source":"def test_on_six_rows():\n    example_argument = np.array([[2081.0, 314942.0], [1059.0, 186606.0],\n                                 [1148.0, 206186.0], [1506.0, 248419.0],\n                                 [1210.0, 214114.0], [1697.0, 277794.0]]\n                                )\n    # Fill in with training array's expected number of rows\n    expected_training_array_num_rows = 4\n    # Fill in with testing array's expected number of rows\n    expected_testing_array_num_rows = 2\n    actual = split_into_training_and_testing_sets(example_argument)\n    # Write the assert statement checking training array's number of rows\n    assert actual[0].shape[0] == 4, \"The actual number of rows in the training array is not {}\".format(expected_training_array_num_rows)","metadata":{},"cell_type":"code","id":"4a6dc08c-92f4-478f-a5a8-6a80bb71b682","execution_count":null,"outputs":[]},{"source":"def test_on_six_rows():\n    example_argument = np.array([[2081.0, 314942.0], [1059.0, 186606.0],\n                                 [1148.0, 206186.0], [1506.0, 248419.0],\n                                 [1210.0, 214114.0], [1697.0, 277794.0]]\n                                )\n    # Fill in with training array's expected number of rows\n    expected_training_array_num_rows = 4\n    # Fill in with testing array's expected number of rows\n    expected_testing_array_num_rows = 2\n    actual = split_into_training_and_testing_sets(example_argument)\n    # Write the assert statement checking training array's number of rows\n    assert actual[0].shape[0] == expected_training_array_num_rows, \"The actual number of rows in the training array is not {}\".format(expected_training_array_num_rows)\n    # Write the assert statement checking testing array's number of rows\n    assert actual[1].shape[0] == expected_testing_array_num_rows, \"The actual number of rows in the testing array is not {}\".format(expected_testing_array_num_rows)","metadata":{},"cell_type":"code","id":"cd46fab1-5c50-4f1e-a781-6becfa4fba1d","execution_count":null,"outputs":[]},{"source":"## Practice the context manager\nIn pytest, you can test whether a function raises an exception by using a context manager. Let's practice your understanding of this important context manager, the with statement and the as clause.\n\nAt any step, feel free to run the code by pressing the \"Run Code\" button and check if the output matches your expectations.","metadata":{},"cell_type":"markdown","id":"24fc76bc-13c1-4ce6-9c2b-3d483d040e61"},{"source":"import pytest\n\n# Fill in with a context manager that will silence the ValueError\nwith pytest.raises(ValueError):\n    raise ValueError","metadata":{},"cell_type":"code","id":"b17b0b70-1bdb-4ac3-a283-73c878d01723","execution_count":null,"outputs":[]},{"source":"import pytest\n\ntry:\n    # Fill in with a context manager that raises Failed if no OSError is raised\n    with pytest.raises(OSError):\n        raise ValueError\nexcept:\n    print(\"pytest raised an exception because no OSError was raised in the context.\")","metadata":{},"cell_type":"code","id":"c3eac56f-84e4-43d5-8bf0-28f228f3128a","execution_count":null,"outputs":[]},{"source":"import pytest\n\n# Store the raised ValueError in the variable exc_info\nwith pytest.raises(ValueError) as exc_info:\n    raise ValueError(\"Silence me!\")","metadata":{},"cell_type":"code","id":"336054b1-dadf-4a9f-9aec-02915ff32a04","execution_count":null,"outputs":[]},{"source":"import pytest\n\nwith pytest.raises(ValueError) as exc_info:\n    raise ValueError(\"Silence me!\")\n# Check if the raised ValueError contains the correct message\nassert exc_info.match(\"Silence me!\")","metadata":{},"cell_type":"code","id":"d3d772d4-131b-4aec-902b-04c1824e79d1","execution_count":null,"outputs":[]},{"source":"## Unit test a ValueError\nSometimes, you want a function to raise an exception when called on bad arguments. This prevents the function from returning nonsense results or hard-to-interpret exceptions. This is an important behavior which should be unit tested.\n\nRemember the function split_into_training_and_testing_sets()? It takes a NumPy array containing housing area and prices as argument. The function randomly splits the array row wise into training and testing arrays in the ratio 3:1, and returns the resulting arrays in a tuple.\n\nIf the argument array has only 1 row, the testing array will be empty. To avoid this situation, you want the function to not return anything, but raise a ValueError with the message \"Argument data_array must have at least 2 rows, it actually has just 1\".","metadata":{},"cell_type":"markdown","id":"b15c36ba-48e6-4694-aea2-2c79f51801a8"},{"source":"import numpy as np\nimport pytest\nfrom train import split_into_training_and_testing_sets\n\ndef test_on_one_row():\n    test_argument = np.array([[1382.0, 390167.0]])\n    # Fill in with a context manager for checking ValueError\n    with pytest.raises(ValueError):\n      split_into_training_and_testing_sets(test_argument)","metadata":{},"cell_type":"code","id":"502b8896-4beb-4510-a276-784572f5d347","execution_count":null,"outputs":[]},{"source":"import numpy as np\nimport pytest\nfrom train import split_into_training_and_testing_sets\n\ndef test_on_one_row():\n    test_argument = np.array([[1382.0, 390167.0]])\n    # Store information about raised ValueError in exc_info\n    with pytest.raises(ValueError) as exc_info:\n      split_into_training_and_testing_sets(test_argument)","metadata":{},"cell_type":"code","id":"319a84f0-3fc9-49e7-83e0-b89ed658da4f","execution_count":null,"outputs":[]},{"source":"import numpy as np\nimport pytest\nfrom train import split_into_training_and_testing_sets\n\ndef test_on_one_row():\n    test_argument = np.array([[1382.0, 390167.0]])\n    # Store information about raised ValueError in exc_info\n    with pytest.raises(ValueError) as exc_info:\n      split_into_training_and_testing_sets(test_argument)\n    expected_error_msg = \"Argument data_array must have at least 2 rows, it actually has just 1\"\n    # Check if the raised ValueError contains the correct message\n    assert exc_info.match(expected_error_msg)","metadata":{},"cell_type":"code","id":"907a0a49-35e5-4cff-ba0b-d336516f7ac8","execution_count":null,"outputs":[]},{"source":"## Testing well: Boundary values\nRemember row_to_list()? It takes a row containing housing area and prices e.g. \"2,041\\t123,781\\n\" and returns the data as a list e.g. [\"2,041\", \"123,781\"].\n\nA row can be mapped to a 2-tuple (m, n), where m is the number of tab separators. n is 1 if the row has any missing values, and 0 otherwise.\n\nFor example,\n\n\"123\\t456\\n\" \n (1, 0).\n\"\\t456\\n\" \n (1, 1).\n\"\\t456\\t\\n\" \n (2, 1).\nThe function only returns a list for arguments mapping to (1, 0). All other tuples correspond to invalid rows, with either more than one tab or missing values. The function returns None in all these cases. See the plot.\n\nThis mapping shows that the function has normal behavior at (1, 0), and special behavior everywhere else.","metadata":{},"cell_type":"markdown","id":"bcc70bd6-0d63-4188-ae48-cd60c25789f2"},{"source":"import pytest\nfrom preprocessing_helpers import row_to_list\n\ndef test_on_no_tab_no_missing_value():    # (0, 0) boundary value\n    # Assign actual to the return value for the argument \"123\\n\"\n    actual = row_to_list(\"123\\n\")\n    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)","metadata":{},"cell_type":"code","id":"94a1d87b-b04c-4038-9285-eee05615611b","execution_count":null,"outputs":[]},{"source":"import pytest\nfrom preprocessing_helpers import row_to_list\n\ndef test_on_no_tab_no_missing_value():    # (0, 0) boundary value\n    # Assign actual to the return value for the argument \"123\\n\"\n    actual = row_to_list(\"123\\n\")\n    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n    \ndef test_on_two_tabs_no_missing_value():    # (2, 0) boundary value\n    actual = row_to_list(\"123\\t4,567\\t89\\n\")\n    # Complete the assert statement\n    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)","metadata":{},"cell_type":"code","id":"ec896f6b-9f55-4707-be2b-54360c616e41","execution_count":null,"outputs":[]},{"source":"import pytest\nfrom preprocessing_helpers import row_to_list\n\ndef test_on_no_tab_no_missing_value():    # (0, 0) boundary value\n    # Assign actual to the return value for the argument \"123\\n\"\n    actual = row_to_list(\"123\\n\")\n    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n    \ndef test_on_two_tabs_no_missing_value():    # (2, 0) boundary value\n    actual = row_to_list(\"123\\t4,567\\t89\\n\")\n    # Complete the assert statement\n    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n    \ndef test_on_one_tab_with_missing_value():    # (1, 1) boundary value\n    actual = row_to_list(\"\\t4,567\\n\")\n    # Format the failure message\n    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)","metadata":{},"cell_type":"code","id":"5c637fe2-15e6-40b0-b7e2-abdc1db598bd","execution_count":null,"outputs":[]},{"source":"## Testing well: Values triggering special logic\nLook at the plot. The boundary values of row_to_list() are now marked in orange. The normal argument is marked in green and the values triggering special behavior are marked in blue.\n\nIn the last exercise, you wrote tests for boundary values. In this exercise, you are going to write tests for values triggering special behavior, in particular, (0, 1) and (2, 1). These are values triggering special logic since the function returns None instead of a list.","metadata":{},"cell_type":"markdown","id":"531aa9e8-6480-4049-aff6-3885b2aff55b"},{"source":"import pytest\nfrom preprocessing_helpers import row_to_list\n\ndef test_on_no_tab_with_missing_value():    # (0, 1) case\n    # Assign to the actual return value for the argument \"\\n\"\n    actual = row_to_list(\"\\n\")\n    # Write the assert statement with a failure message\n    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n    \ndef test_on_two_tabs_with_missing_value():    # (2, 1) case\n    # Assign to the actual return value for the argument \"123\\t\\t89\\n\"\n    actual = row_to_list(\"123\\t\\t89\\n\")\n    # Write the assert statement with a failure message\n    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)","metadata":{},"cell_type":"code","id":"b1f9cce6-1582-407f-ab9b-71bc6898b240","execution_count":null,"outputs":[]},{"source":"## Testing well: Normal arguments\nThis time, you will test row_to_list() with normal arguments i.e. arguments mapping to the tuple (1, 0). The plot is provided to you for reference.\n\nRemembering that the best practice is to test for two to three normal arguments, you will write two tests in this exercise.","metadata":{},"cell_type":"markdown","id":"dcd8efe4-89c6-4a46-8718-4b3eebd5d8ba"},{"source":"import pytest\nfrom preprocessing_helpers import row_to_list\n\ndef test_on_normal_argument_1():\n    actual = row_to_list(\"123\\t4,567\\n\")\n    # Fill in with the expected return value for the argument \"123\\t4,567\\n\"\n    expected = [\"123\", \"4,567\"]\n    assert actual == expected, \"Expected: {0}, Actual: {1}\".format(expected, actual)\n    \ndef test_on_normal_argument_2():\n    actual = row_to_list(\"1,059\\t186,606\\n\")\n    expected = [\"1,059\", \"186,606\"]\n    # Write the assert statement along with a failure message\n    assert actual == expected, \"Expected: {0}, Actual: {1}\".format(expected, actual)","metadata":{},"cell_type":"code","id":"dc54e2f6-f8de-489f-be71-2426805bb44e","execution_count":null,"outputs":[]},{"source":"## TDD: Tests for normal arguments\nIn this and the following exercises, you will implement the function convert_to_int() using Test Driven Development (TDD). In TDD, you write the tests first and implement the function later.\n\nNormal arguments for convert_to_int() are integer strings with comma as thousand separators. Since the best practice is to test a function for two to three normal arguments, here are three examples with no comma, one comma and two commas respectively.\n\nArgument value\tExpected return value\n\"756\"\t756\n\"2,081\"\t2081\n\"1,034,891\"\t1034891\nSince the convert_to_int() function does not exist yet, you won't be able to import it. But you will use it in the tests anyway. That's how TDD works.\n\npytest has already been imported for you.","metadata":{},"cell_type":"markdown","id":"b979bde1-929c-44c3-b505-50f6bc92503a"},{"source":"def test_with_no_comma():\n    actual = convert_to_int(\"756\")\n    # Complete the assert statement\n    assert actual == 756, \"Expected: 756, Actual: {0}\".format(actual)\n    \ndef test_with_one_comma():\n    actual = convert_to_int(\"2,081\")\n    # Complete the assert statement\n    assert actual == 2081, \"Expected: 2081, Actual: {0}\".format(actual)\n    \ndef test_with_two_commas():\n    actual = convert_to_int(\"1,034,891\")\n    # Complete the assert statement\n    assert actual == 1034891, \"Expected: 1034891, Actual: {0}\".format(actual)","metadata":{},"cell_type":"code","id":"a2adf8b1-7fbf-48db-bd21-9847bbb831d7","execution_count":null,"outputs":[]},{"source":"## TDD: Requirement collection\nWhat should convert_to_int() do if the arguments are not normal? In particular, there are three special argument types:\n\nArguments that are missing a comma e.g. \"178100,301\".\nArguments that have the comma in the wrong place e.g. \"12,72,891\".\nFloat valued strings e.g. \"23,816.92\".\nAlso, should convert_to_int() raise an exception for specific argument values?\n\nWhen your boss asked you to implement the function, she didn't say anything about these cases! But since you want to write tests for special and bad arguments as a part of TDD, you go and ask your boss.\n\nShe says that convert_to_int() should return None for every special argument and there are no bad arguments for this function.\n\npytest has been imported for you.","metadata":{},"cell_type":"markdown","id":"0182770e-81b0-4f9d-bd9e-487071225d44"},{"source":"# Give a name to the test for an argument with missing comma\ndef test_on_string_with_missing_comma():\n    actual = convert_to_int(\"178100,301\")\n    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n    \ndef test_on_string_with_incorrectly_placed_comma():\n    # Assign to the actual return value for the argument \"12,72,891\"\n    actual = convert_to_int(\"12,72,891\")\n    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n    \ndef test_on_float_valued_string():\n    actual = convert_to_int(\"23,816.92\")\n    # Complete the assert statement\n    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)","metadata":{},"cell_type":"code","id":"d1d19407-9685-475c-af28-faa62e873df3","execution_count":null,"outputs":[]},{"source":"## TDD: Implement the function\nconvert_to_int() returns None for the following:\n\nArguments with missing thousands comma e.g. \"178100,301\". If you split the string at the comma using \"178100,301\".split(\",\"), then the resulting list [\"178100\", \"301\"] will have at least one entry with length greater than 3 e.g. \"178100\".\n\nArguments with incorrectly placed comma e.g. \"12,72,891\". If you split this at the comma, then the resulting list is [\"12\", \"72\", \"891\"]. Note that the first entry is allowed to have any length between 1 and 3. But if any other entry has a length other than 3, like \"72\", then there's an incorrectly placed comma.\n\nFloat valued strings e.g. \"23,816.92\". If you remove the commas and call int() on this string i.e. int(\"23816.92\"), you will get a ValueError.","metadata":{},"cell_type":"markdown","id":"54a26ce5-11ff-488f-aee2-ca724377d6e8"},{"source":"def convert_to_int(integer_string_with_commas):\n    comma_separated_parts = integer_string_with_commas.split(\",\")\n    for i in range(len(comma_separated_parts)):\n        # Write an if statement for checking missing commas\n        if len(comma_separated_parts[i]) > 3:\n            return None\n        # Write the if statement for incorrectly placed commas\n        if i != 0 and len(comma_separated_parts[i]) != 3:\n            return None\n    integer_string_without_commas = \"\".join(comma_separated_parts)\n    try:\n        return int(integer_string_without_commas)\n    # Fill in with the correct exception for float valued argument strings\n    except ValueError:\n        return None","metadata":{},"cell_type":"code","id":"fe48638c-ad68-4bf1-a1f6-02140af2390f","execution_count":null,"outputs":[]},{"source":"## Create a test class\nTest classes are containers inside test modules. They help separate tests for different functions within the test module, and serve as a structuring tool in the pytest framework.\n\nTest classes are written in CamelCase e.g. TestMyFunction as opposed to tests, which are written using underscores e.g. test_something().\n\nYou met the function split_into_training_and_testing_sets() in Chapter 2, and wrote some tests for it. One of these tests was called test_on_one_row() and it checked if the function raises a ValueError when passed a NumPy array with only one row.\n\nIn this exercise you are going to create a test class for this function. This test class will hold the test test_on_one_row().","metadata":{},"cell_type":"markdown","id":"1ac2ad6b-5a7d-450d-b138-7d5bded00e55"},{"source":"import pytest\nimport numpy as np\n\nfrom models.train import split_into_training_and_testing_sets\n\n# Declare the test class\nclass TestSplitIntoTrainingAndTestingSets(object):\n    # Fill in with the correct mandatory argument\n    def test_on_one_row(self):\n        test_argument = np.array([[1382.0, 390167.0]])\n        with pytest.raises(ValueError) as exc_info:\n            split_into_training_and_testing_sets(test_argument)\n        expected_error_msg = \"Argument data_array must have at least 2 rows, it actually has just 1\"\n        assert exc_info.match(expected_error_msg)","metadata":{},"cell_type":"code","id":"bcdd374f-dab6-4846-adf1-3a0c4bf41794","execution_count":null,"outputs":[]},{"source":"## Running test classes\nWhen you ran the !pytest command in the last exercise, the test test_on_six_rows() failed. This is a test for the function split_into_training_and_testing_sets(). This means that this function is broken.\n\nShort recap in case you forgot: this function takes a NumPy array containing housing area and prices as argument. The function randomly splits the argument array into training and testing arrays in the ratio 3:1, and returns the resulting arrays in a tuple.\n\nA quick look revealed that during the code update, someone inadvertently changed the split from 3:1 to 9:1. This has to be changed back and the unit tests for the function, which now lives in the test class TestSplitIntoTrainingAndTestingSets, needs to be run again. Are you up to the challenge?","metadata":{},"cell_type":"markdown","id":"2dae7f3c-53ad-4b4e-a7cb-aac8efc7af4e"},{"source":"import numpy as np\n\ndef split_into_training_and_testing_sets(data_array):\n    dim = data_array.ndim\n    if dim != 2:\n        raise ValueError(\"Argument data_array must be two dimensional. Got {0} dimensional array instead!\".format(dim))\n    num_rows = data_array.shape[0]\n    if num_rows < 2:\n        raise ValueError(\"Argument data_array must have at least 2 rows, it actually has just {0}\".format(num_rows))\n    # Fill in with the correct float\n    num_training = int(0.33 * data_array.shape[0])\n    permuted_indices = np.random.permutation(data_array.shape[0])\n    return data_array[permuted_indices[:num_training], :], data_array[permuted_indices[num_training:], :]","metadata":{},"cell_type":"code","id":"b98b709e-a332-4509-8298-40aed52dcbaa","execution_count":null,"outputs":[]},{"source":"## Mark a test class as expected to fail\nA new function model_test() is being developed and it returns the accuracy of a given linear regression model on a testing dataset. Test Driven Development (TDD) is being used to implement it. The procedure is: write tests first and then implement the function.\n\nA test class TestModelTest has been created within the test module models/test_train.py. In the test class, there are two unit tests called test_on_linear_data() and test_on_one_dimensional_array(). But the function model_test() has not been implemented yet.\n\nThroughout this exercise, pytest and numpy as np will be imported for you.","metadata":{},"cell_type":"markdown","id":"b4f108a0-40d3-4e1a-a9de-2ca357ad01e5"},{"source":"# Mark the whole test class as \"expected to fail\"\n@pytest.mark.xfail\nclass TestModelTest(object):\n    def test_on_linear_data(self):\n        test_input = np.array([[1.0, 3.0], [2.0, 5.0], [3.0, 7.0]])\n        expected = 1.0\n        actual = model_test(test_input, 2.0, 1.0)\n        message = \"model_test({0}) should return {1}, but it actually returned {2}\".format(test_input, expected, actual)\n        assert actual == pytest.approx(expected), message\n        \n    def test_on_one_dimensional_array(self):\n        test_input = np.array([1.0, 2.0, 3.0, 4.0])\n        with pytest.raises(ValueError) as exc_info:\n            model_test(test_input, 1.0, 1.0)","metadata":{},"cell_type":"code","id":"13128c49-ea3e-4173-97a4-d254132fad66","execution_count":null,"outputs":[]},{"source":"# Add a reason for the expected failure\n@pytest.mark.xfail(reason=\"Using TDD, model_test() has not yet been implemented\")\nclass TestModelTest(object):\n    def test_on_linear_data(self):\n        test_input = np.array([[1.0, 3.0], [2.0, 5.0], [3.0, 7.0]])\n        expected = 1.0\n        actual = model_test(test_input, 2.0, 1.0)\n        message = \"model_test({0}) should return {1}, but it actually returned {2}\".format(test_input, expected, actual)\n        assert actual == pytest.approx(expected), message\n        \n    def test_on_one_dimensional_array(self):\n        test_input = np.array([1.0, 2.0, 3.0, 4.0])\n        with pytest.raises(ValueError) as exc_info:\n            model_test(test_input, 1.0, 1.0)","metadata":{},"cell_type":"code","id":"908184d9-90ea-4b65-8d58-0d37e3bf0ba2","execution_count":null,"outputs":[]},{"source":"## Mark a test as conditionally skipped\nIn Python 2, there was a built-in function called xrange(). In Python 3, xrange() was removed. Therefore, if any test uses xrange(), it's going to fail with a NameError in Python 3.\n\nRemember the function get_data_as_numpy_array()? You saw it in Chapter 2. It converted data in a preprocessed data file into a NumPy array.\n\nrange() has been deliberately replaced with the obsolete xrange() in the function. Evil laughter! But no worries, it will be changed back after you're done with this exercise.\n\nYou wrote a test called test_on_clean_file() for this function. This test currently resides in a test class TestGetDataAsNumpyArray inside the test module features/test_as_numpy.py.\n\npytest, numpy as np and get_data_as_numpy_array() has been imported for you.","metadata":{},"cell_type":"markdown","id":"e6e6b52a-d186-4124-90ce-6975820d806b"},{"source":"# Import the sys module\nimport sys\n\nclass TestGetDataAsNumpyArray(object):\n    # Add a reason for skipping the test\n    @pytest.mark.skipif(sys.version_info > (2, 7), reason=\"Works only on Python 2.7 or lower\")\n    def test_on_clean_file(self):\n        expected = np.array([[2081.0, 314942.0],\n                             [1059.0, 186606.0],\n                             [1148.0, 206186.0]\n                             ]\n                            )\n        actual = get_data_as_numpy_array(\"example_clean_data.txt\", num_columns=2)\n        message = \"Expected return value: {0}, Actual return value: {1}\".format(expected, actual)\n        assert actual == pytest.approx(expected), message","metadata":{},"cell_type":"code","id":"b2cecc48-eaa8-41d0-bd74-9b80f4525bb2","execution_count":null,"outputs":[]},{"source":"## Use a fixture for a clean data file\nIn the video, you saw how the preprocess() function creates a clean data file.\n\nThe get_data_as_numpy_array() function takes the path to this clean data file as the first argument and the number of columns of data as the second argument. It returns a NumPy array holding the data.\n\nIn a previous exercise, you wrote the test test_on_clean_file() without using a fixture. That's bad practice! This time, you'll use the fixture clean_data_file(), which\n\ncreates a clean data file in the setup,\nyields the path to the clean data file,\nremoves the clean data file in the teardown.\nThe contents of the clean data file that you will use for testing is printed in the IPython console.\n\npytest, os, numpy as np and get_data_as_numpy_array() have been imported for you.","metadata":{},"cell_type":"markdown","id":"c8c523fc-9fed-41db-897e-1b5a25aca9c7"},{"source":"# Add a decorator to make this function a fixture\n@pytest.fixture\ndef clean_data_file():\n    file_path = \"clean_data_file.txt\"\n    with open(file_path, \"w\") as f:\n        f.write(\"201\\t305671\\n7892\\t298140\\n501\\t738293\\n\")\n    yield file_path\n    os.remove(file_path)\n    \n# Pass the correct argument so that the test can use the fixture\ndef test_on_clean_file(clean_data_file):\n    expected = np.array([[201.0, 305671.0], [7892.0, 298140.0], [501.0, 738293.0]])\n    # Pass the clean data file path yielded by the fixture as the first argument\n    actual = get_data_as_numpy_array(clean_data_file, 2)\n    assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual) ","metadata":{},"cell_type":"code","id":"e975533f-aea7-48ef-9a1d-1a78c3353fad","execution_count":null,"outputs":[]},{"source":"## Write a fixture for an empty data file\nWhen a function takes a data file as an argument, you need to write a fixture that takes care of creating and deleting that data file. This exercise will test your ability to write such a fixture.\n\nget_data_as_numpy_array() should return an empty numpy array if it gets an empty data file as an argument. To test this behavior, you need to write a fixture empty_file() that does the following.\n\nCreates an empty data file empty.txt relative to the current working directory in setup.\nYields the path to the empty data file.\nDeletes the empty data file in teardown.\nThe fixture will be used by the test test_on_empty_file(), which is available for you to see in the script.\n\nos, pytest, numpy as np and get_data_as_numpy_array have been imported for you.","metadata":{},"cell_type":"markdown","id":"a80fbc63-b100-45b4-af2f-0e46434b440f"},{"source":"@pytest.fixture\ndef empty_file():\n    # Assign the file path \"empty.txt\" to the variable\n    file_path = \"empty.txt\"\n    open(file_path, \"w\").close()\n    # Yield the variable file_path\n    yield file_path\n    # Remove the file in the teardown\n    os.remove(file_path)\n    \ndef test_on_empty_file(self, empty_file):\n    expected = np.empty((0, 2))\n    actual = get_data_as_numpy_array(empty_file, 2)\n    assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual)","metadata":{},"cell_type":"code","id":"ed8ecfc7-5d8b-4882-81c0-5a1020791251","execution_count":null,"outputs":[]},{"source":"## Fixture chaining using tmpdir\nThe built-in tmpdir fixture is very useful when dealing with files in setup and teardown. tmpdir combines seamlessly with user defined fixture via fixture chaining.\n\nIn this exercise, you will use the power of tmpdir to redefine and improve the empty_file() fixture that you wrote in the last exercise and get some experience with fixture chaining.","metadata":{},"cell_type":"markdown","id":"d348d4d5-9bdd-435e-961b-fc5c88b139d5"},{"source":"import pytest\n\n@pytest.fixture\n# Add the correct argument so that this fixture can chain with the tmpdir fixture\ndef empty_file(tmpdir):\n    # Use the appropriate method to create an empty file in the temporary directory\n    file_path = tmpdir.join(\"empty.txt\")\n    open(file_path, \"w\").close()\n    yield file_path","metadata":{},"cell_type":"code","id":"5f92f804-f6c7-4126-b1f5-b451646e62c4","execution_count":null,"outputs":[]},{"source":"## Program a bug-free dependency\nIn the video, row_to_list() was mocked. But preprocess() has another dependency convert_to_int(). Generally, its best to mock all dependencies of the function under test. It's your job to mock convert_to_int() in this and the following exercises.\n\nThe raw data file used in the test is printed in the IPython console. The second row \"1,767565,112\\n\" is dirty, so row_to_list() will filter it out. The rest will be converted to lists and convert_to_int() will process the areas and prices.\n\nThe mocked convert_to_int() should process these areas and prices correctly. Here is the dictionary holding the correct return values.\n\n{\"1,801\": 1801, \"201,411\": 201411, \"2,002\": 2002, \"333,209\": 333209, \"1990\": None, \"782,911\": 782911, \"1,285\": 1285, \"389129\": None}","metadata":{},"cell_type":"markdown","id":"ee7a0e81-6a95-42ac-9444-e5b3514f963a"},{"source":"# Define a function convert_to_int_bug_free\ndef convert_to_int_bug_free(comma_separated_integer_string):\n    # Assign to the dictionary holding the correct return values \n    return_values = {\"1,801\": 1801, \"201,411\": 201411, \"2,002\": 2002, \"333,209\": 333209, \"1990\": None, \"782,911\": 782911, \"1,285\": 1285, \"389129\": None}\n    # Return the correct result using the dictionary return_values\n    return return_values[comma_separated_integer_string]","metadata":{},"cell_type":"code","id":"e4a24b32-d610-47b7-969d-dbb5b1e47cbc","execution_count":null,"outputs":[]},{"source":"## Mock a dependency\nMocking helps us replace a dependency with a MagicMock() object. Usually, the MagicMock() is programmed to be a bug-free version of the dependency. To verify whether the function under test works properly with the dependency, you simply check whether the MagicMock() is called with the correct arguments and in the right order.\n\nIn the last exercise, you programmed a bug-free version of the dependency data.preprocessing_helpers.convert_to_int in the context of the test test_on_raw_data(), which applies preprocess() on a raw data file. The data file is printed out in the IPython console.\n\npytest, unittest.mock.call, preprocess raw_and_clean_data_file and convert_to_int_bug_free has been imported for you.","metadata":{},"cell_type":"markdown","id":"b62ab2b0-a412-46c3-9a35-2e7ef6e56ee0"},{"source":"# Add the correct argument to use the mocking fixture in this test\ndef test_on_raw_data(self, raw_and_clean_data_file, mocker):\n    raw_path, clean_path = raw_and_clean_data_file\n    # Replace the dependency with the bug-free mock\n    convert_to_int_mock = mocker.patch(\"data.preprocessing_helpers.convert_to_int\",\n                                       side_effect=convert_to_int_bug_free)\n    preprocess(raw_path, clean_path)\n    # Check if preprocess() called the dependency correctly\n    assert convert_to_int_mock.call_args_list == [call(\"1,801\"), call(\"201,411\"), call(\"2,002\"), call(\"333,209\"), call(\"1990\"), call(\"782,911\"), call(\"1,285\"), call(\"389129\")]\n    with open(clean_path, \"r\") as f:\n        lines = f.readlines()\n    first_line = lines[0]\n    assert first_line == \"1801\\\\t201411\\\\n\"\n    second_line = lines[1]\n    assert second_line == \"2002\\\\t333209\\\\n\" ","metadata":{},"cell_type":"code","id":"bc29fe0d-e32e-406c-9642-77e8a758408f","execution_count":null,"outputs":[]},{"source":"## Testing on linear data\nThe model_test() function, which measures how well the model fits unseen data, returns a quantity called \n which is very difficult to compute in the general case. Therefore, you need to find special testing sets where computing \n is easy.\n\nOne important special case is when the model fits the testing set perfectly. This happens when the testing set is perfectly linear. One such testing set is printed out in the IPython console for you.\n\nIn this special case, model_test() should return 1.0 if the model's slope and intercept match the testing set, because 1.0 is usually the highest possible value that \n can take.\n\nRemember that for data points \n, the slope is \n \n and the intercept is \n.\n### Instructions\n\nAssign the variable test_argument to a NumPy array holding the perfectly linear testing data printed out in the IPython console.\nAssign the variable expected to the expected value of \n in the special case of a perfect fit.\nFill in with the model's slope and intercept that matches the testing set.\nRemembering that actual is a float, complete the assert statement to check if actual returned by model_test() is equal to the expected return value expected.\n\n","metadata":{},"cell_type":"markdown","id":"5ece2ed4-5950-4db7-9a66-19ee049af0a3"},{"source":"import numpy as np\nimport pytest\nfrom models.train import model_test\n\ndef test_on_perfect_fit():\n    # Assign to a NumPy array containing a linear testing set\n    test_argument = np.array([[1.0, 3.0], [2.0, 5.0], [3.0, 7.0]])\n    # Fill in with the expected value of r^2 in the case of perfect fit\n    expected = 1.0\n    # Fill in with the slope and intercept of the model\n    actual = model_test(test_argument, slope=2.0, intercept=1.0)\n    # Complete the assert statement\n    assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual)","metadata":{},"cell_type":"code","id":"23378572-b3a0-4781-bc74-ae7036cb2ec0","execution_count":null,"outputs":[]},{"source":"## Testing on circular data\nAnother special case where it is easy to guess the value of \n is when the model does not fit the testing dataset at all. In this case, \n takes its lowest possible value 0.0.\n\nThe plot shows such a testing dataset and model. The testing dataset consists of data arranged in a circle of radius 1.0. The x and y co-ordinates of the data is shown on the plot. The model corresponds to a straight line y=0.\n\nAs one can easily see, the straight line does not fit the data at all. In this particular case, the value of \n is known to be 0.0.\n\nYour job is to write a test test_on_circular_data() for the function model_test() that performs this sanity check. pytest, numpy as np, model_test, sin, cos and pi have been imported for you.","metadata":{},"cell_type":"markdown","id":"1971d887-af0e-4a5a-abb7-4bf6c8bcf20c"},{"source":"def test_on_circular_data(self):\n    theta = pi/4.0\n    # Assign to a NumPy array holding the circular testing data\n    test_argument = np.array([[1.0, 0.0], [cos(theta), sin(theta)],\n                              [0.0, 1.0],\n                              [cos(3 * theta), sin(3 * theta)],\n                              [-1.0, 0.0],\n                              [cos(5 * theta), sin(5 * theta)],\n                              [0.0, -1.0],\n                              [cos(7 * theta), sin(7 * theta)]]\n                             )\n    # Fill in with the slope and intercept of the straight line\n    actual = model_test(test_argument, slope=0.0, intercept=0.0)\n    # Complete the assert statement\n    assert actual == pytest.approx(0.0)","metadata":{},"cell_type":"code","id":"a708e374-bd24-4ec1-8a62-f8d569af4ee0","execution_count":null,"outputs":[]},{"source":"## Generate the baseline image\nIn this exercise, you will get one step closer to the real thing. During this whole course, you've built a library of tests using a Python script and an IPython console. In real life, you're more likely to use an IDE (Integrated Development Environment), that lets you write scripts in the language you want, organize them into your directories, and execute shell commands. Basically, an IDE increases your productivity by gathering the most common activities of software development into a single application: writing source code, executing, and debugging.\n\nHere, you can see the directory you've built on the left pane. The upper right pane is where you will write your Python scripts, and the bottom right pane is a shell console, which replaces the IPython console you've used so far.\n\nParts of an integrated development environment\n\nIn this exercise, you will test the function introduced in the video get_plot_for_best_fit_line() on another set of test arguments. Here is the test data.\n\n1.0    3.0\n2.0    8.0\n3.0    11.0\nThe best fit line that the test will draw follows the equation \n. Two points, (1.0, 3.0) and (2.0, 8.0) will fall on the line. The point (3.0, 11.0) won't. The title of the plot will be \"Test plot for almost linear data\".\n\nThe test is called test_plot_for_almost_linear_data() and it's your job to complete the test and generate the baseline image. pytest, numpy as np and get_plot_for_best_fit_line has been imported for you.","metadata":{},"cell_type":"markdown","id":"174a926c-2438-4ab6-a957-06a78225cc3e"},{"source":"import pytest\nimport numpy as np\n\nfrom visualization.plots import get_plot_for_best_fit_line\n\nclass TestGetPlotForBestFitLine(object):\n    # Add the pytest marker which generates baselines and compares images\n    @pytest.mark.mpl_image_compare\n    def test_plot_for_almost_linear_data(self):\n        slope = 5.0\n        intercept = -2.0\n        x_array = np.array([1.0, 2.0, 3.0])\n        y_array = np.array([3.0, 8.0, 11.0])\n        title = \"Test plot for almost linear data\"\n        # Return the matplotlib figure returned by the function under test\n        return get_plot_for_best_fit_line(slope, intercept, x_array, y_array, title)\n        ","metadata":{},"cell_type":"code","id":"0d8ae543-ac34-4a77-bba3-a1ac1b7db39c","execution_count":null,"outputs":[]},{"source":"## Fix the plotting function\nIn the last exercise, pytest saved the baseline images, actual images, and images containing the pixelwise difference in a temporary folder. The difference image for one of the tests test_on_almost_linear_data() is shown below.\n\n\n\nThe black areas are where the actual image and the baseline matches. The white areas are where they don't match.\n\nThis clearly tells us that something is wrong with the axis labels. Take a look at the plots section to see the baseline (plot 1/2) and the actual plot (plot 2/2). Based on that, it's your job to fix the plotting function.","metadata":{},"cell_type":"markdown","id":"8bff14dd-6d8a-4366-b7a3-92231725693c"},{"source":"import matplotlib.pyplot as plt\nimport numpy as np\n\ndef get_plot_for_best_fit_line(slope, intercept, x_array, y_array, title):\n    fig, ax = plt.subplots()\n    ax.plot(x_array, y_array, \".\")\n    ax.plot([0, np.max(x_array)], [intercept, slope * np.max(x_array) + intercept], \"-\")\n    # Fill in with axis labels so that they match the baseline\n    ax.set(xlabel='area (square feet)', ylabel='price (dollars)', title=title)\n    return fig","metadata":{},"cell_type":"code","id":"06bee702-aff9-40d9-b157-2e7c38d81595","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}