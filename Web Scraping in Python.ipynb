{"cells":[{"source":"# Web Scraping in Python","metadata":{},"id":"20c73b4c-626a-4674-b2ca-0f4c47f60628","cell_type":"markdown"},{"source":"## Keep it Classy\nIn this two-part exercise, you will have a chance to show off what you've learned about attributes; in this case, we focus on the class attribute.\nFill in the blank in the HTML code string html to assign a class attribute to the second div element which has the value \"you-are-classy\".","metadata":{},"cell_type":"markdown","id":"e199294e-7552-448d-87e2-3a33da074d02"},{"source":"# HTML code string\nhtml = '''\n<html>\n  <body>\n    <div class=\"class1\" id=\"div1\">\n      <p class=\"class2\">Visit DataCamp!</p>\n    </div>\n    <div class=\"you-are-classy\">\n      <p class=\"class2\">Keep up the good work!</p>\n    </div>\n  </body>\n</html>\n'''\n# Print out the class of the second div element\nwhats_my_class( html )","metadata":{},"cell_type":"code","id":"c1d884f9-b4a6-45b8-af64-f1a2889495aa","outputs":[],"execution_count":null},{"source":"## Where it's @\nIn this exercise, you'll begin to write an XPath string using attributes to achieve a certain task; that task is to select the paragraph element containing the text \"Thanks for Watching!\". We've already created most of the XPath string for you.\n\nConsider the following HTML:\n\n<html>\n  <body>\n    <div id=\"div1\" class=\"class-1\">\n      <p class=\"class-1 class-2\">Hello World!</p>\n      <div id=\"div2\">\n        <p id=\"p2\" class=\"class-2\">Choose DataCamp!</p>\n      </div>\n    </div>\n    <div id=\"div3\" class=\"class-2\">\n      <p class=\"class-2\">Thanks for Watching!</p>\n    </div>\n  </body>\n</html>\nWe have created the function print_element_text() for you, which will print any text contained in your element.","metadata":{},"cell_type":"markdown","id":"bb8890b5-c275-4855-a57d-37fd3c47360c"},{"source":"# Create an Xpath string to select desired p element\nxpath = '//*[@id=\"div3\"]/p'\n\n# Print out selection text\nprint_element_text( xpath )","metadata":{},"cell_type":"code","id":"bd817b09-0115-4d84-847f-c67549e6ff8c","outputs":[],"execution_count":null},{"source":"## Check your Class\nThis exercise is to emphasize that when you use an XPath to select an element by its class attribute without using the contains() function, you match the class exactly. Your job is to fill in the blank below and finish the variable xpath directing to the specified element.\n\nConsider the following HTML:\n\n<html>\n  <body>\n    <div id=\"div1\" class=\"class-1\">\n      <p class=\"class-1 class-2\">Hello World!</p>\n      <div id=\"div2\">\n        <p id=\"p2\" class=\"class-2\">Choose DataCamp!</p>\n      </div>\n    </div>\n    <div id=\"div3\" class=\"class-2\">\n      <p class=\"class-2\">Thanks for Watching!</p>\n    </div>\n  </body>\n</html>","metadata":{},"cell_type":"markdown","id":"b31a2f59-fa18-47af-802c-544ea1b5db33"},{"source":"# Create an XPath string to select p element by class\nxpath = '//p[@class=\"class-1 class-2\"]'\n\n# Print out select text\nprint_element_text( xpath )","metadata":{},"cell_type":"code","id":"c842fd3d-c266-4ec7-a1e8-a80be09cda36","outputs":[],"execution_count":null},{"source":"## Hyper(link) Active\nOne of the most important attributes to extract for \"web-crawling\" is the hyperlink url (href attribute) within an a tag. Here, you will extract such a hyperlink! We have created the function print_attribute to print out the data extracted from your XPath, so you can test your XPath strings in the console, if you like.\n\nThe exercise refers to the following HTML source code:\n\n<html>\n  <body>\n    <div id=\"div1\" class=\"class-1\">\n      <p class=\"class-1 class-2\">Hello World!</p>\n      <div id=\"div2\">\n        <p id=\"p2\" class=\"class-2\">Choose \n            <a href=\"http://datacamp.com\">DataCamp!</a>!\n        </p>\n      </div>\n    </div>\n    <div id=\"div3\" class=\"class-2\">\n      <p class=\"class-2\">Thanks for Watching!</p>\n    </div>\n  </body>\n</html>","metadata":{},"cell_type":"markdown","id":"a4027652-1b32-41f5-9a2a-7f31fb7062f9"},{"source":"# Create an xpath to the href attribute\nxpath = '//p[@id=\"p2\"]/a/@href'\n\n# Print out the selection(s); there should be only one\nprint_attribute( xpath )","metadata":{},"cell_type":"code","id":"91d4cf43-0c36-450f-8260-ec74cf2245f7","outputs":[],"execution_count":null},{"source":"## Secret Links\nWe have loaded the HTML from a secret website and have used it to create the functions how_many_elements() and preview(). The function how_many_elements() allows you to pass in an XPath string and it will print out the number of elements the XPath you wrote has selected. The function preview() allows you to pass in an XPath string and it will print out the first few elements you've selected.\n\nYour job in this exercise is to create an XPath which directs to all href attribute values of the hyperlink a elements whose class attributes contain the string \"package-snippet\". If you do it correctly, you should find that you have selected 10 elements with your XPath string and that it previews links.","metadata":{},"cell_type":"markdown","id":"2903edd0-27d6-46d9-8785-ae68c64e12c1"},{"source":"# Create an xpath to the href attributes\nxpath = '//a[contains(@href,\"package-snippet\")]/@href'\n\n# Print out how many elements are selected\nhow_many_elements( xpath )\n# Preview the selected elements\npreview( xpath )","metadata":{},"cell_type":"code","id":"505d3cad-d1a7-493b-9031-92e385037b97","outputs":[],"execution_count":null},{"source":"## Divvy Up This Exercise\nWe have pre-loaded an HTML into the string variable html. In this two part problem you will use this html variable as the HTML document to set up a Selector object with, and create a SelectorList which selects all div elements; then, you will check your understanding of what happens within the SelectorList.","metadata":{},"cell_type":"markdown","id":"5b01b94b-f450-46ff-95fb-e6db1563a17c"},{"source":"from scrapy import Selector\n\n# Create a Selector selecting html as the HTML document\nsel = Selector( text=html )\n\n# Create a SelectorList of all div elements in the HTML document\ndivs = sel.xpath( '//div' )","metadata":{},"cell_type":"code","id":"2f26c2a4-eaa7-4389-be13-16fc431555a2","outputs":[],"execution_count":null},{"source":"## Requesting a Selector\nWe have pre-loaded the URL for a particular website in the string variable url and use the requests library to put the content from the website into the string variable html. Your task is to create a Selector object sel using the HTML source code stored in html.","metadata":{},"cell_type":"markdown","id":"e9ceb666-6b62-4254-abc8-dc6ff4b01b3a"},{"source":"# Import a scrapy Selector\nfrom scrapy import Selector\n\n# Import requests\nimport requests\n\n# Create the string html containing the HTML source\nhtml = requests.get( url ).content\n\n# Create the Selector object sel from html\nsel = Selector( text=html )\n\n# Print out the number of elements in the HTML document\nprint( \"There are 1020 elements in the HTML document.\")\nprint( \"You have found: \", len( sel.xpath('//*') ) )","metadata":{},"cell_type":"code","id":"719f24d7-e799-4518-b69b-f67617e2d1bc","outputs":[],"execution_count":null},{"source":"## The (X)Path to CSS Locators\nMany people prefer using CSS Locator notation to XPath notation. As we will see later, it often makes attribute selection very easy. To help get you more comfortable going back and forth between XPath and CSS Locator strings, we give you a chance in this exercise to do some direct \"translation\" between the two.\n\nNote that the exercises in this chapter may take some time to load.","metadata":{},"cell_type":"markdown","id":"38236f5d-0544-437c-9595-5a5a595f0d5d"},{"source":"# Create the XPath string equivalent to the CSS Locator \nxpath = '/html/body/span[1]//a'\n\n# Create the CSS Locator string equivalent to the XPath\ncss_locator = 'html>body>span:nth-of-type(1) a'\n\n# Create the XPath string equivalent to the CSS Locator \nxpath = '//div[@id=\"uid\"]/span//h4'\n\n# Create the CSS Locator string equivalent to the XPath\ncss_locator = 'div#uid > span h4'","metadata":{},"cell_type":"code","id":"602c7f96-eada-4618-9aab-6d8c2bd3a950","outputs":[],"execution_count":null},{"source":"## Get an \"a\" in this Course\nWe have loaded the HTML from a secret website which you will use to set up a Selector object and the function how_many_elements(). When passing this function a CSS Locator string, it will print out the number of elements that the CSS Locator you wrote has selected.\n\nIn the second part of this problem, we want you to create a CSS Locator string which will select a certain collection of elements as described here: Select the hyperlink (a element) children of all div elements belonging to the class \"course-block\" (that is, any div element with a class attribute such that \"course-block\" is one of the classes assigned). The number of such elements is 11, so you can check your solution with how_many_elements if you choose.","metadata":{},"cell_type":"markdown","id":"c183881e-9c49-47ee-bb81-3c46aec8b76a"},{"source":"from scrapy import Selector\n\n# Create a selector from the html (of a secret website)\nsel = Selector( text = html )\n\n# Fill in the blank\ncss_locator = 'div.course-block>a'\n\n# Print the number of selected elements.\nhow_many_elements( css_locator )","metadata":{},"cell_type":"code","id":"e5490c8f-bdab-44ad-9628-4ea611735500","outputs":[],"execution_count":null},{"source":"## You've been `href`ed\nIn a previous exercise, you created a CSS Locator string to select the hyperlink (a element) children of all div elements belonging to the class \"course-block\". Here we have created a SelectorList called course_as having selected those hyperlink children.\n\nNow, we want you to fill in the blank below to extract the href attribute values from these elements. This is another example of chaining, as we've seen in a previous exercise.\n\nThe point here is that we can chain together calls to the methods css and xpath, and combine them! We help nudge you in the correct direction by giving you the solution if we chain with another call to the css method.","metadata":{},"cell_type":"markdown","id":"b36ae517-0961-4cb9-b0a1-cbca023fa1e4"},{"source":"from scrapy import Selector\n\n# Create a selector object from a secret website\nsel = Selector( text=html )\n\n# Select all hyperlinks of div elements belonging to class \"course-block\"\ncourse_as = sel.css( 'div.course-block > a' )\n\n# Selecting all href attributes chaining with css\nhrefs_from_css = course_as.css( '::attr(href)' )\n\n# Selecting all href attributes chaining with xpath\nhrefs_from_xpath = course_as.xpath('./@href').extract()","metadata":{},"cell_type":"code","id":"0bf87a85-131a-49d7-b810-9affc7d3ef7f","outputs":[],"execution_count":null},{"source":"## Top Level Text\nThis exercise will have you write an XPath and CSS Locator string to direct to the text of a specific paragraph p element. The p element in the HTML is uniquely defined by its id attribute, which is \"p3\". With this small piece of information, you should be able to create the desired strings; however, we have preloaded the variable html with a string containing the HTML in which this link belongs, if you want to peruse it.\n\nIn this exercise, you will only be selecting the text within the element, which does not include the text in future generations of the element. We have created a function print_results for you to compare which elements your strings direct to.","metadata":{},"cell_type":"markdown","id":"04794704-0265-4bb3-b8f4-beeb5a5925c3"},{"source":"# Create an XPath string to the desired text.\nxpath =  \"//p[@id='p3']/text()\"\n\n# Create a CSS Locator string to the desired text.\ncss_locator = \"p#p3::text\"\n\n# Print the text from our selections\nprint_results( xpath, css_locator )","metadata":{},"cell_type":"code","id":"9e28faaa-af24-4a88-babd-80c6197924f8","outputs":[],"execution_count":null},{"source":"## All Level Text\nThis exercise is similar to the previous, but differs in that you will be selecting text from multiple generations of a given element.\n\nYou will write an XPath and CSS Locator strings to direct to the text of a specific paragraph p element. The p element in the HTML is uniquely defined by its id attribute, which is \"p3\". With this small piece of information, you should be able to create the desired strings; however, we have preloaded the variable html with a string containing the HTML in which this link belongs, if you want to peruse it.\n\nIn this exercise, you will only be selecting the text within the element which includes all text within the future generations. We have created a function print_results for you to compare which elements your strings direct to.","metadata":{},"cell_type":"markdown","id":"69403493-d8bc-4c57-9bc4-dacd5387e1e2"},{"source":"# Create an XPath string to the desired text.\nxpath = '//p[@id=\"p3\"]//text()'\n\n\n# Create a CSS Locator string to the desired text.\ncss_locator = 'p#p3 ::text'\n\n# Print the text from our selections\nprint_results( xpath, css_locator )\n","metadata":{},"cell_type":"code","id":"6648f7ca-18a4-4319-9a29-a70f3aac6663","outputs":[],"execution_count":null},{"source":"## Reveal By Response\nWe have pre-loaded a Response object, named response, with the content from a secret website. Your job is to figure out the URL and the title of the website using the response variable. You learned how to find the URL in the last lesson. To find the website title, what you need to know is:\n\nThe title is the text from the title element\nThe title element is a child of the head element, which is a child of the html root element.\nTo note: the html root element only has one child head element, and the head element only has one child title element.","metadata":{},"cell_type":"markdown","id":"067e4fe9-5e0f-40de-876b-c33588977b6b"},{"source":"# Get the URL to the website loaded in response\nthis_url = response.url\n\n# Get the title of the website loaded in response\nthis_title = response.xpath('/html/head/title/text()').extract_first()\n\n# Print out our findings\nprint_url_title( this_url, this_title )","metadata":{},"cell_type":"code","id":"a4ac95d8-0a6c-43a8-ab5d-c3fd84e5f3da","outputs":[],"execution_count":null},{"source":"## Responding with Selectors\nSomething that we should emphasize at this point about the relationship between a Selector and Response objects is that both objects return a SelectorList when using the xpath or css methods to direct to elements. In this exercise, we'll prove it to you, by having you find all hyperlink elements belonging to the class course-block__link (notice the double underscore!) and looking at the object that is produced when doing so.\n\nRecall that to find an element by class, you can use a period (.). For example, div.class-2 selects all div elements belonging to class-2.\n\nWe have pre-loaded both a Response object named response and a Selector object named sel with the content from the same \"secret\" website. Once you complete the task of creating a CSS Locator, you will compare both the output from response.css and selector.css to see that they are effectively the same!","metadata":{},"cell_type":"markdown","id":"1dd21a6d-04ec-47ce-a8d2-4b04af44b80c"},{"source":"# Create a CSS Locator string to the desired hyperlink elements\ncss_locator = 'a.course-block__link'\n\n# Select the hyperlink elements from response and sel\nresponse_as = response.css(css_locator)\nsel_as = sel.css(css_locator)\n\n# Examine similarity\nnr = len( response_as )\nns = len( sel_as )\nfor i in range( min(nr, ns, 2) ):\n  print( \"Element %d from response: %s\" % (i+1, response_as[i]) )\n  print( \"Element %d from sel: %s\" % (i+1, sel_as[i]) )\n  print( \"\" )","metadata":{},"cell_type":"code","id":"7fbd91c2-9fc5-4504-9c38-11d055ccba7b","outputs":[],"execution_count":null},{"source":"## Selecting from a Selection\nIn this exercise, you will find the text from an h4 element within a particular div element. It will occur in steps where the first step is selecting a family of div elements, and the second step is narrowing in on the first one, from which we will grab the h4 element text. This process of progressively narrowing in on elements (e.g., first to the div elements, then to the h4 element) is another example of \"chaining\", even if it doesn't look exactly the same as we've seen it before.\n\nAlong the way in this exercise, there is a variable first_div set up for you to use. Think carefully about what type of object first_div is!","metadata":{},"cell_type":"markdown","id":"bb5b49c2-22e4-41b0-ba5c-6bab143952d7"},{"source":"# Select all desired div elements\ndivs = response.css('div.course-block')\n\n# Take the first div element\nfirst_div = divs[0]\n\n# Extract the text from the (only) h4 element in first_div\nh4_text = first_div.css('h4::text').extract_first()\n\n# Print out the text\nprint( \"The text from the h4 element is:\", h4_text )","metadata":{},"cell_type":"code","id":"c12bbaf5-f559-41ed-b392-ffd6e3069542","outputs":[],"execution_count":null},{"source":"## Titular\nSimilar to the work given in the previous lesson, we will have you use a pre-loaded Response object, named response to scrape the course titles from the (shortened version of the) DataCamp course directory https://www.datacamp.com/courses/all. To successfully do so, you only need to know the following\n\nThe course titles are the text from all the h4 elements within the HTML document.\nWe ask you to extract these course titles here.","metadata":{},"cell_type":"markdown","id":"6f030869-ded5-42bb-b938-e0d045473710"},{"source":"# Create a SelectorList of the course titles\ncrs_title_els = response.css('h4::text')\n\n# Extract the course titles \ncrs_titles = crs_title_els.extract()\n\n# Print out the course titles \nfor el in crs_titles:\n  print( \">>\", el )","metadata":{},"cell_type":"code","id":"ae6ee73a-db22-4ef1-a876-2706a26dc60c","outputs":[],"execution_count":null},{"source":"## Scraping with Children\nWe did a cute trick in the lesson to calculate how many children there were of one of the div elements belonging to the class course-block. Here we ask you to find the number of children of a mystery element (already stored within a Selector object, so you can use the xpath or css method).\n\nTo be explicit, we have created the Selector object mystery in the following way:\n\nWe first loaded a Response variable using a secret website as the input.\nThen we used a call to the xpath method to create a SelectorList of elements (but we won't say which ones)\nFinally, we let mystery be the first Selector object of this SelectorList.","metadata":{},"cell_type":"markdown","id":"a962ee9f-bce3-4dcd-802d-26bb6d869db1"},{"source":"# Calculate the number of children of the mystery element\nhow_many_kids = len( mystery.xpath( './*' ) )\n\n# Print out the number\nprint( \"The number of elements you selected was:\", how_many_kids )","metadata":{},"cell_type":"code","id":"c66372a5-9fca-456e-97e6-e35c9d3277f9","outputs":[],"execution_count":null},{"source":"## nheriting the Spider\nWhen learning about scrapy spiders, we saw that the main portion of the code for us to adjust is the class for the spider. To help build some familiarity of the class, you will complete a short piece of code to complete a toy-model of the spider class code. We've omitted the code that would actually run the spider, only including the pieces necessary to create the class.\n\nAs mentioned in the lesson, a class is roughly a collection of related variables and functions housed together. Sometimes one class likes to use methods from another class, and so we will inherit methods from a different class. That's what we do in the spider class.\n\nWe wrote the function inspect_class to look at the your class once you're done, if you'd like to test your solution!","metadata":{},"cell_type":"markdown","id":"c21aaae0-375e-4544-bdff-db67386bac6b"},{"source":"# Import scrapy library\nimport scrapy\n\n# Create the spider class\nclass YourSpider(scrapy.Spider):\n  name = \"your_spider\"\n  # start_requests method\n  def start_requests(self):\n    pass\n  # parse method\n  def parse(self, response):\n    pass\n  \n# Inspect Your Class\ninspect_class(YourSpider)","metadata":{},"cell_type":"code","id":"53ba0815-804f-4127-b227-0adb9146146d","outputs":[],"execution_count":null},{"source":"## Hurl the URLs\nIn the next lesson we will talk about the start_requests method within the spider class. In this quick exercise, we ask you to change around a variable within the start_requests method which foreshadows some of what we will be learning in the next lesson. Basically, we want you to start becoming comfortable turning some of the wheels within a spider class; in this case, making a list of urls within the start_requests method.\n\nWe've written a function inspect_class which will print out the list of elements you have in the urls variable within the start_requests method.\n\nNote: in the next several exercises, you will write code to complete your spider class, but the code does not yet include the pieces to actually run the spider; that will come at the end.","metadata":{},"cell_type":"markdown","id":"a39a1664-6897-440a-815e-fa42fd2efb85"},{"source":"# Import scrapy library\nimport scrapy\n\n# Create the spider class\nclass YourSpider( scrapy.Spider ):\n  name = \"your_spider\"\n  # start_requests method\n  def start_requests( self ):\n    urls = [\"https://www.datacamp.com\",\"https://scrapy.org\"]\n    for url in urls:\n      yield url\n  # parse method\n  def parse( self, response ):\n    pass\n  \n# Inspect Your Class\ninspect_class( YourSpider )","metadata":{},"cell_type":"code","id":"9d68278b-2c9f-43a2-bf5e-f509cf3668c3","outputs":[],"execution_count":null},{"source":"## Self Referencing is Classy\nYou probably have noticed that within the spider class, we always input the argument self in the start_requests and parse methods (just look in the sample code in this exercise!). This allows us to reference between methods within the class. That is, if we want to refer to the method parse within the start_requests method, we would need to write self.parse rather than just parse; what writing self does is tell the code: \"Look in the same class as start_requests for a method called parse to use.\"\n\nIn this exercise you will get a chance to play with this \"self referencing\".","metadata":{},"cell_type":"markdown","id":"4430b414-a3ea-43fc-91bd-3b4d9494e3c7"},{"source":"# Import scrapy library\nimport scrapy\n\n# Create the spider class\nclass YourSpider( scrapy.Spider ):\n  name = \"your_spider\"\n  # start_requests method\n  def start_requests( self ):\n    self.print_msg( \"Hello World!\")\n  # parse method\n  def parse( self, response ):\n    pass\n  # print_msg method\n  def print_msg( self, msg ):\n    print( \"Calling start_requests in YourSpider prints out:\", msg )\n  \n# Inspect Your Class\ninspect_class( YourSpider )","metadata":{},"cell_type":"code","id":"99c785de-16b4-4dbf-a643-bbd2f290f26a","outputs":[],"execution_count":null},{"source":"## Starting with Start Requests\nIn the last lesson we learned about setting up the start_requests method within a scrapy spider. Here we have another toy-model spider which doesn't actually scrape anything, but gives you a chance to play with the start_requests method. What we want is for you to start becomming familiar with the arguments you pass into the scrapy.Request call within start_requests.\n\nAs before, we have created the function inspect_class to examine what you are yielding in start_requests.","metadata":{},"cell_type":"markdown","id":"aec05959-fafa-4d29-8d54-dc8ea612be93"},{"source":"# Import scrapy library\nimport scrapy\n\n# Create the spider class\nclass YourSpider( scrapy.Spider ):\n  name = \"your_spider\"\n  # start_requests method\n  def start_requests( self ):\n    yield scrapy.Request( url=\"https://www.datacamp.com\", callback=self.parse )\n  # parse method\n  def parse( self, response ):\n    pass\n  \n# Inspect Your Class\ninspect_class( YourSpider )","metadata":{},"cell_type":"code","id":"53dc96bf-e3b3-4a33-8180-a8550592dcd5","outputs":[],"execution_count":null},{"source":"## Pen Names\nIn this exercise, we have set up a spider class which, when finished, will retrieve the author names from a shortened version of the DataCamp course directory. The URL for the shortened version is stored in the variable url_short. Your job will be to create the list of extracted author names in the parse method of the spider.\n\nTwo things you should know:\n\nYou will be using the response object and the css method here.\nThe course author names are defined by the text within the paragraph p elements belonging to the class course-block__author-name\nYou can inspect the spider using the function inspect_spider() that we built for you -- it will print out the author names you find!\n\nNote that this and the remaining exercises in this chapter may take some time to load.","metadata":{},"cell_type":"markdown","id":"eb923c29-2c2f-47b6-97d9-c90494a399c3"},{"source":"# Import the scrapy library\nimport scrapy\n\n# Create the Spider class\nclass DCspider( scrapy.Spider ):\n  name = 'dcspider'\n  # start_requests method\n  def start_requests( self ):\n    yield scrapy.Request( url = url_short, callback = self.parse )\n  # parse method\n  def parse( self, response ):\n    # Create an extracted list of course author names\n    author_names = response.css('p.course-block__author-name::text').extract()\n    # Here we will just return the list of Authors\n    return author_names\n  \n# Inspect the spider\ninspect_spider( DCspider )","metadata":{},"cell_type":"code","id":"9dd0f011-a612-44c0-8ad5-54a446575957","outputs":[],"execution_count":null},{"source":"## Crawler Time\nThis will be your first chance to play with a spider which will crawl between sites (by first collecting links from one site, and following those links to parse new sites). This spider starts at the shortened DataCamp course directory, then extracts the links of the courses in the parse method; from there, it will follow those links to extract the course descriptions from each course page in the parse_descr method, and put these descriptions into the list course_descrs. Your job is to complete the code so that the spider runs as desired!\n\nWe have created a function inspect_spider which will print out one of the course descriptions you scrape (if done correctly)!","metadata":{},"cell_type":"markdown","id":"8fa3e742-9657-4e35-90d6-10687fc27377"},{"source":"# Import the scrapy library\nimport scrapy\n\n# Create the Spider class\nclass DCdescr( scrapy.Spider ):\n  name = 'dcdescr'\n  # start_requests method\n  def start_requests( self ):\n    yield scrapy.Request( url = url_short, callback = self.parse )\n  \n  # First parse method\n  def parse( self, response ):\n    links = response.css( 'div.course-block > a::attr(href)' ).extract()\n    # Follow each of the extracted links\n    for link in links:\n      yield response.follow( url = link, callback = self.parse_descr )\n      \n  # Second parsing method\n  def parse_descr( self, response ):\n    # Extract course description\n    course_descr = response.css( 'p.course__description::text' ).extract_first()\n    # For now, just yield the course description\n    yield course_descr\n\n\n# Inspect the spider\ninspect_spider( DCdescr )","metadata":{},"cell_type":"code","id":"be8d57b3-727f-4e06-bc54-c96bd7b36a63","outputs":[],"execution_count":null},{"source":"## Time to Run\nIn the last lesson, we went through creating an entire web-crawler to access course information from each course in the DataCamp course directory. However, the lesson seemed to stop without a climax, because we didn't play with the code after finishing the parsing methods.\n\nThe point of this exercise is to remedy that!\n\nThe code we give you to look at in this and the next exercise is long, because its the entire spider that took us the lesson to create! However, don't be intimidated! The point of these two exercises is to give you a very easy task to complete, with the hope that you will look at and run the code for this spider. That way, even though it is long, you will have a grasp of it!","metadata":{},"cell_type":"markdown","id":"0b1cec44-3b6d-414b-996c-035ed899a700"},{"source":"# Import scrapy\nimport scrapy\n\n# Import the CrawlerProcess: for running the spider\nfrom scrapy.crawler import CrawlerProcess\n\n# Create the Spider class\nclass DC_Chapter_Spider(scrapy.Spider):\n  name = \"dc_chapter_spider\"\n  # start_requests method\n  def start_requests(self):\n    yield scrapy.Request(url = url_short,\n                         callback = self.parse_front)\n  # First parsing method\n  def parse_front(self, response):\n    course_blocks = response.css('div.course-block')\n    course_links = course_blocks.xpath('./a/@href')\n    links_to_follow = course_links.extract()\n    for url in links_to_follow:\n      yield response.follow(url = url,\n                            callback = self.parse_pages)\n  # Second parsing method\n  def parse_pages(self, response):\n    crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n    crs_title_ext = crs_title.extract_first().strip()\n    ch_titles = response.css('h4.chapter__title::text')\n    ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n    dc_dict[ crs_title_ext ] = ch_titles_ext\n\n# Initialize the dictionary **outside** of the Spider class\ndc_dict = dict()\n\n# Run the Spider\nprocess = CrawlerProcess()\nprocess.crawl(DC_Chapter_Spider)\nprocess.start()\n\n# Print a preview of courses\npreviewCourses(dc_dict)","metadata":{},"cell_type":"code","id":"2f3bc961-412a-4b5d-a66f-ef20f1b3c99b","outputs":[],"execution_count":null},{"source":"## DataCamp Descriptions\nLike the previous exercise, the code here is long since you are working with an entire web-crawling spider! But again, don't let the amount of code intimidate you, you have a handle on how spiders work now, and you are perfectly capable to complete the easy task for you here!\n\nAs in the previous exercise, we have created a function previewCourses which lets you preview the output of the spider, but you can always just explore the dictionary dc_dict too after you run the code.\n\nIn this exercise, you are asked to create a CSS Locator string direct to the text of the course description. All you need to know is that from the course page, the course description text is within a paragraph p element which belongs to the class course__description (two underlines).","metadata":{},"cell_type":"markdown","id":"6b807153-4eee-470f-812a-defb580c9011"},{"source":"# Import scrapy\nimport scrapy\n\n# Import the CrawlerProcess: for running the spider\nfrom scrapy.crawler import CrawlerProcess\n\n# Create the Spider class\nclass DC_Description_Spider(scrapy.Spider):\n  name = \"dc_chapter_spider\"\n  # start_requests method\n  def start_requests(self):\n    yield scrapy.Request(url = url_short,\n                         callback = self.parse_front)\n  # First parsing method\n  def parse_front(self, response):\n    course_blocks = response.css('div.course-block')\n    course_links = course_blocks.xpath('./a/@href')\n    links_to_follow = course_links.extract()\n    for url in links_to_follow:\n      yield response.follow(url = url,\n                            callback = self.parse_pages)\n  # Second parsing method\n  def parse_pages(self, response):\n    # Create a SelectorList of the course titles text\n    crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n    # Extract the text and strip it clean\n    crs_title_ext = crs_title.extract_first().strip()\n    # Create a SelectorList of course descriptions text\n    crs_descr = response.css('p.course__description::text')\n    # Extract the text and strip it clean\n    crs_descr_ext = crs_descr.extract_first().strip()\n    # Fill in the dictionary\n    dc_dict[crs_title_ext] = crs_descr_ext\n\n# Initialize the dictionary **outside** of the Spider class\ndc_dict = dict()\n\n# Run the Spider\nprocess = CrawlerProcess()\nprocess.crawl(DC_Description_Spider)\nprocess.start()\n\n# Print a preview of courses\npreviewCourses(dc_dict)","metadata":{},"cell_type":"code","id":"553caf6f-8644-4491-98b8-24eea1d42a83","outputs":[],"execution_count":null},{"source":"## Capstone Crawler\nThis exercise gives you a chance to show off what you've learned! In this exercise, you will write the parse function for a spider and then fill in a few blanks to finish off the spider. On the course directory page of DataCamp, each listed course has a title and a short course description. This spider will be used to scrape the course directory to extract the course titles and short course descriptions. You will not need to follow any links this time. Everything you need to know is:\n\nThe course titles are defined by the text within an h4 element whose class contains the string block__title (double underline).\nThe short course descriptions are defined by the text within a paragraph p element whose class contains the string block__description (double underline).","metadata":{},"cell_type":"markdown","id":"5a6df423-2d9d-4b3d-a878-c571efc5bc4c"},{"source":"# Import scrapy\nimport scrapy\n\n# Import the CrawlerProcess\nfrom scrapy.crawler import CrawlerProcess\n\n# Create the Spider class\nclass YourSpider(scrapy.Spider):\n  name = 'yourspider'\n  # start_requests method\n  def start_requests(self):\n    yield scrapy.Request(url = url_short, callback = self.parse)\n      \n  def parse(self, response):\n    # My version of the parser you wrote in the previous part\n    crs_titles = response.xpath('//h4[contains(@class,\"block__title\")]/text()').extract()\n    crs_descrs = response.xpath('//p[contains(@class,\"block__description\")]/text()').extract()\n    for crs_title, crs_descr in zip(crs_titles, crs_descrs):\n      dc_dict[crs_title] = crs_descr\n    \n# Initialize the dictionary **outside** of the Spider class\ndc_dict = dict()\n\n# Run the Spider\nprocess = CrawlerProcess()\nprocess.crawl(YourSpider)\nprocess.start()\n\n# Print a preview of courses\npreviewCourses(dc_dict)","metadata":{},"cell_type":"code","id":"a7653596-8136-46ae-8efe-409e5c1aadf0","outputs":[],"execution_count":null}],"metadata":{"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}